# ==============================================================================
# === 1. IMPORTA√á√ïES DE BIBLIOTERAS (REVISADO PARA NUVEM)
# ==============================================================================
import logging
import streamlit as st
import copy
from openai import OpenAI
import json
from difflib import SequenceMatcher
import fitz  # PyMuPDF
import docx
from dotenv import load_dotenv
import os
import datetime
import random
import re
import base64
import pandas as pd
import plotly.express as px
from fpdf import FPDF
from auth import check_password  # Sua autentica√ß√£o local
from utils import carregar_preferencias, salvar_preferencias
import joblib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import requests
import io
# As bibliotecas librosa, io (se usado apenas para √°udio), soundfile, PyAudio foram removidas
# do contexto do microfone e processamento de √°udio em tempo real.
from fpdf.enums import XPos, YPos
from openai import RateLimitError
import time
from supabase import create_client, Client
from pathlib import Path
from utils import encrypt_file_content_general, decrypt_file_content_general
from utils import carregar_dados_do_github, salvar_dados_no_github, decrypt_file_content_general, encrypt_file_content_general
from utils import salvar_emocoes, carregar_emocoes
from datetime import datetime


# ‚úÖ Bloco de ping para manter o app acordado
params = st.query_params
if "ping" in params:
    st.write("‚úÖ Jarvis IA est√° online!")
    st.stop()
# ==============================================================================
# === 2. VERIFICA√á√ÉO DE LOGIN E CONFIGURA√á√ÉO INICIAL
# ==============================================================================
ADMIN_USERNAME = st.secrets.get("ADMIN_USERNAME", os.getenv("ADMIN_USERNAME"))
if not ADMIN_USERNAME:
    st.error(
        "Nome de usu√°rio admin n√£o encontrado! Defina ADMIN_USERNAME em .env ou secrets.")
    st.stop()

# Executa a verifica√ß√£o de login primeiro
if not check_password():
    st.stop()  # Interrompe a execu√ß√£o do script se o login falhar

# --- Define visibilidade padr√£o do campo de feedback ---
# Ajustado para que o feedback possa ser sempre vis√≠vel se desejar, ou controlado por admin
if st.session_state.get("username") != ADMIN_USERNAME:
    st.session_state["show_feedback_form"] = True
else:
    st.session_state["show_feedback_form"] = True

emocoes_dict = {} # Inicializa como dicion√°rio vazio para evitar erros caso n√£o carregue
ultima_emocao = None
if st.session_state.username:
    emocoes_dict_carregadas = carregar_emocoes(st.session_state.username)
    if emocoes_dict_carregadas:
        emocoes_dict = emocoes_dict_carregadas
        try:
            latest_timestamp = max(emocoes_dict.keys(), key=lambda k: datetime.fromisoformat(k))
            latest_entry = emocoes_dict[latest_timestamp]
            # --- IN√çCIO DA MODIFICA√á√ÉO NECESS√ÅRIA ---
            if isinstance(latest_entry, dict):
                ultima_emocao = latest_entry.get("emocao", "neutro").lower()
            else: # Formato antigo (apenas string)
                ultima_emocao = str(latest_entry).lower()
            # --- FIM DA MODIFICA√á√ÉO NECESS√ÅRIA ---
        except Exception as e:
            print(f"Erro ao obter a √∫ltima emo√ß√£o carregada: {e}")
            ultima_emocao = None
# ==============================================================================
# === 3. CONEX√ÉO INTELIGENTE DE API (LOCAL E NUVEM)
# ==============================================================================

# Carrega as vari√°veis do arquivo .env (para o ambiente local)
load_dotenv()

# Verifica se a chave est√° nos "Secrets" do Streamlit (quando est√° na nuvem)
if "OPENAI_API_KEY" in st.secrets:
    # Ambiente da Nuvem
    st.sidebar.success("Jarvis Online", icon="‚òÅÔ∏è")
    api_key = st.secrets["OPENAI_API_KEY"]
    # Usamos .get() para n√£o dar erro se n√£o existir
    api_key_serper = st.secrets.get("SERPER_API_KEY")
else:
    # Ambiente Local
    st.sidebar.info("Jarvis Online", icon="‚òÅÔ∏è")
    api_key = os.getenv("OPENAI_API_KEY")
    api_key_serper = os.getenv("SERPER_API_KEY")

# Valida√ß√£o para garantir que a chave de API foi carregada
if not api_key:
    st.error(
        "Chave de API da OpenAI n√£o encontrada! Verifique seu arquivo .env ou os Secrets na nuvem.")
    st.stop()

# Inicializa o modelo da OpenAI com a chave correta
modelo = OpenAI(api_key=api_key)


def chamar_openai_com_retries(modelo_openai, mensagens, modelo="gpt-4o", max_tentativas=3, pausa_segundos=5):
    """
    Faz a chamada √† API da OpenAI com tentativas autom√°ticas em caso de RateLimitError.
    """
    for tentativa in range(1, max_tentativas + 1):
        try:
            st.info(f"‚è≥ Um instante... (consulta {tentativa}) em andamento")
            resposta = modelo_openai.chat.completions.create(
                model=modelo,
                messages=mensagens
            )
            return resposta  # sucesso!
        except RateLimitError:
            st.warning(
                f"‚ö†Ô∏è Limite de requisi√ß√µes atingido. Tentando novamente em {pausa_segundos} segundos...")
            time.sleep(pausa_segundos)
        except Exception as e:
            st.error(f"‚ùå Erro inesperado: {e}")
            break

    st.error("‚ùå Tentativas esgotadas. Aguardando voc√™ tentar novamente mais tarde.")
    return None

# ==============================================================================
# === 4. CONFIGURA√á√ÉO DE LOGS
# ==============================================================================


def setup_logging():
    """Configura o sistema de log para registrar eventos em um arquivo."""
    logging.basicConfig(
        filename='jarvis_log.txt',
        filemode='a',  # 'a' para adicionar ao arquivo, 'w' para sobrescrever
        format='%(asctime)s - %(levelname)s - %(message)s',
        level=logging.INFO,
        encoding='utf-8'
    )


setup_logging()

# --- CARREGAR O MODELO E FERRAMENTAS ---


@st.cache_resource
def carregar_modelo_embedding():
    """Carrega o modelo de embedding que √© pesado e n√£o muda."""
    print("Executando CARGA PESADA do modelo de embedding (isso s√≥ deve aparecer uma vez)...")
    try:
        return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    except Exception as e:
        print(f"Erro fatal ao carregar o modelo de embedding: {e}")
        return None


def inicializar_memoria_dinamica():
    """Carrega os vetores e a base de conhecimento no estado da sess√£o, se ainda n√£o estiverem l√°."""
    if 'vetores_perguntas' not in st.session_state:
        print("Inicializando mem√≥ria din√¢mica na sess√£o...")
        try:
            st.session_state.vetores_perguntas = np.load(
                'vetores_perguntas_v2.npy')
            st.session_state.base_de_conhecimento = joblib.load(
                'dados_conhecimento_v2.joblib')
            print("Mem√≥ria din√¢mica carregada com sucesso.")
        except Exception as e:
            print(f"Erro ao carregar arquivos de mem√≥ria (.npy, .joblib): {e}")
            st.session_state.vetores_perguntas = None
            st.session_state.base_de_conhecimento = None


# --- CARREGAR O MODELO E INICIALIZAR A MEM√ìRIA ---
modelo_embedding = carregar_modelo_embedding()
inicializar_memoria_dinamica()  # Garante que a mem√≥ria est√° pronta na sess√£o

# Exibe a mensagem de status no painel lateral
if modelo_embedding:
    st.sidebar.success("Mem√≥ria ativada.", icon="üíæ")
else:
    st.sidebar.error("Arquivos do modelo local n√£o encontrados.")

# --- Fun√ß√µes do Aplicativo ---

# <--- FUN√á√ÉO OTIMIZADA: Substitui detectar_emocao, detectar_tom_usuario e classificar_categoria
def analisar_metadados_prompt(prompt_usuario):
    """
    Analisa o prompt do usu√°rio com uma √∫nica chamada √† IA para extrair m√∫ltiplos metadados.
    Retorna um dicion√°rio com emo√ß√£o, sentimento, categoria e tipo de intera√ß√£o.
    """
    if not prompt_usuario or not prompt_usuario.strip():
        return {
            "emocao": "neutro", "sentimento_usuario": "n/a",
            "categoria": "geral", "tipo_interacao": "conversa_geral"
        }

    prompt_analise = f"""
    Analise o texto do usu√°rio e extraia as seguintes informa√ß√µes em um objeto JSON:
    1. "emocao": A emo√ß√£o principal. Escolha uma de: 'feliz', 'triste', 'irritado', 'neutro', 'curioso', 'grato', 'ansioso', 'confuso', 'surpreso', 'animado', 'preocupado'.
    2. "sentimento_usuario": O tom ou estado de esp√≠rito em poucas palavras (ex: 'apressado', 'curioso', 'frustrado').
    3. "categoria": Uma categoria simples para o t√≥pico (ex: 'geografia', 'programa√ß√£o', 'sentimentos').
    4. "tipo_interacao": Classifique como 'pergunta', 'comando', 'desabafo_apoio' ou 'conversa_geral'.

    Texto do usu√°rio: "{prompt_usuario}"

    Responda APENAS com o objeto JSON.
    """
    try:
        # Usa um modelo mais r√°pido e barato para esta tarefa de classifica√ß√£o simples
        modelo_selecionado = 'gpt-3.5-turbo'
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt_analise}],
            temperature=0,
            response_format={"type": "json_object"}
        )
        return json.loads(resposta_modelo.choices[0].message.content)
    except Exception as e:
        print(f"Erro ao analisar metadados do prompt: {e}")
        # Retorna um dicion√°rio padr√£o em caso de erro
        return {
            "emocao": "neutro", "sentimento_usuario": "n/a",
            "categoria": "geral", "tipo_interacao": "conversa_geral"
        }


def limpar_pdf_da_memoria():
    """Remove os dados do PDF do st.session_state para o bot√£o de download desaparecer."""
    if 'pdf_para_download' in st.session_state:
        del st.session_state['pdf_para_download']
    if 'pdf_filename' in st.session_state:
        del st.session_state['pdf_filename']

# <--- REMOVIDO: A segunda defini√ß√£o duplicada de limpar_pdf_da_memoria foi apagada.

def gerar_conteudo_para_pdf(topico):
    """Usa a IA para gerar um texto bem formatado sobre um t√≥pico para o PDF."""
    prompt = f"Por favor, escreva um texto detalhado e bem estruturado sobre o seguinte t√≥pico para ser inclu√≠do em um documento PDF. Organize com par√°grafos claros e, se apropriado, use listas. T√≥pico: '{topico}'"
    modelo_selecionado = st.session_state.get('admin_model_choice', 'gpt-4o')
    resposta_modelo = modelo.chat.completions.create(
        model=modelo_selecionado,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2048
    )
    return resposta_modelo.choices[0].message.content


def criar_pdf(texto_corpo, titulo_documento):
    """
    Cria um arquivo PDF em mem√≥ria, interpretando formata√ß√£o Markdown
    e usando uma fonte Unicode local de forma robusta e portatil.
    """
    pdf = FPDF()
    pdf.add_page()

    # Constr√≥i o caminho completo e robusto para as fontes
    script_dir = os.path.dirname(__file__)
    font_path_regular = os.path.join(script_dir, 'assets', 'DejaVuSans.ttf')
    font_path_bold = os.path.join(script_dir, 'assets', 'DejaVuSans-Bold.ttf')

    try:
        pdf.add_font('DejaVu', '', font_path_regular)
        pdf.add_font('DejaVu', 'B', font_path_bold)
        FONT_FAMILY = 'DejaVu'
    except Exception: 
        print("AVISO: Arquivos de fonte n√£o encontrados. Verifique a pasta 'assets'. Usando Helvetica.")
        FONT_FAMILY = 'Helvetica'

    pdf.set_font(FONT_FAMILY, 'B', 18)
    pdf.multi_cell(0, 10, titulo_documento, new_x=XPos.LMARGIN,
                   new_y=YPos.NEXT, align='C')
    pdf.ln(15)

    texto_corpo_ajustado = texto_corpo.strip().replace(
        '*\n', '* ').replace('-\n', '- ')
    linhas = texto_corpo_ajustado.split('\n')

    for linha in linhas:
        linha = linha.strip()
        if linha.startswith('### '):
            pdf.set_font(FONT_FAMILY, 'B', 14)
            pdf.multi_cell(0, 8, linha.lstrip('### ').strip(),
                           new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(4)
        elif linha.startswith('## '):
            pdf.set_font(FONT_FAMILY, 'B', 16)
            pdf.multi_cell(0, 10, linha.lstrip('## ').strip(),
                           new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(6)
        elif linha.startswith('# '):
            pdf.set_font(FONT_FAMILY, 'B', 18)
            pdf.multi_cell(0, 12, linha.lstrip('# ').strip(),
                           new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(8)
        # --- ADI√á√ÉO PARA CABE√áALHO H4 (####) ---
        elif linha.startswith('#### '): 
            pdf.set_font(FONT_FAMILY, 'B', 12) 
            pdf.multi_cell(0, 7, linha.lstrip('#### ').strip(),
                           new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(3) 
        # --- FIM DA ADI√á√ÉO ---
        elif re.match(r'^\*\*(.+?)\*\*$', linha): 
            pdf.set_font(FONT_FAMILY, 'B', 12)
            texto_negrito = re.sub(r'^\*\*(.+?)\*\*$', r'\1', linha)
            pdf.multi_cell(0, 7, texto_negrito,
                        new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(3)
        elif linha.startswith('* ') or linha.startswith('- '):
            pdf.set_font(FONT_FAMILY, '', 12)
            bullet = "‚Ä¢" if FONT_FAMILY == 'DejaVu' else "*"
            pdf.cell(8, 7, f"  {bullet} ")
            texto_da_linha = linha.lstrip('* ').lstrip('- ').strip()            
            write_with_mixed_styles(texto_da_linha, pdf, FONT_FAMILY)
            pdf.ln(2)
        elif linha:
            pdf.set_font(FONT_FAMILY, '', 12)            
            write_with_mixed_styles(linha, pdf, FONT_FAMILY)
            pdf.ln(5)

    return bytes(pdf.output())


def write_with_mixed_styles(text, pdf, font_family):
    """
    Escreve uma linha de texto no PDF, alternando entre estilos normal e negrito
    com base na marca√ß√£o '**'.
    """
    parts = text.split('**')
    for i, part in enumerate(parts):
        if i % 2 == 1:
            pdf.set_font(font_family, 'B')
        else:
            pdf.set_font(font_family, '')
        pdf.write(7, part)
    pdf.ln()


def extrair_texto_documento(uploaded_file):
    """Extrai o texto de arquivos PDF, DOCX, TXT, Excel, e v√°rias linguagens de programa√ß√£o e scripts de banco de dados."""
    nome_arquivo = uploaded_file.name

    if nome_arquivo.endswith(('.xlsx', '.xls')):
        try:
            df = pd.read_excel(uploaded_file, engine='openpyxl')
            return df.to_csv(index=False)
        except Exception as e:
            return f"Erro ao ler o arquivo Excel: {e}"
    elif nome_arquivo.endswith(".pdf"):
        texto = ""
        with fitz.open(stream=uploaded_file.read(), filetype="pdf") as doc:
            for page in doc:
                texto += page.get_text()
        return texto
    elif nome_arquivo.endswith(".docx"):
        doc = docx.Document(uploaded_file)
        return "\n".join([p.text for p in doc.paragraphs])
    elif nome_arquivo.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")
    # --- Modifica√ß√£o ABRANGENTE para arquivos de programa√ß√£o e script ---
    elif nome_arquivo.endswith((
        '.py', '.js', '.ts', '.html', '.htm', '.css', '.php', '.java', '.kt',
        '.c', '.cpp', '.h', '.cs', '.rb', '.go', '.swift', '.sql', '.json',
        '.xml', '.yaml', '.yml', '.md', '.sh', '.bat', '.ps1', '.R', '.pl', '.lua'
    )):
        try:
            # Tenta UTF-8 primeiro, que √© o mais comum para c√≥digo
            return uploaded_file.read().decode("utf-8")
        except UnicodeDecodeError:
            # Se UTF-8 falhar, tenta Latin-1 (ou outra codifica√ß√£o comum no seu contexto)
            return uploaded_file.read().decode("latin-1")
        except Exception as e:
            return f"Erro ao ler o arquivo de c√≥digo/script: {e}"
    # --- FIM da modifica√ß√£o ---
    return "Formato de arquivo n√£o suportado."

def gerar_imagem_com_dalle(prompt_para_imagem):
    """
    Gera uma imagem com DALL-E 3 e retorna seus dados em formato Base64.
    """
    try:
        st.info(
            f"üé® Gerando imagem com DALL-E 3 para: '{prompt_para_imagem}'...")
        response = modelo.images.generate(
            model="dall-e-3",
            prompt=prompt_para_imagem,
            size="1024x1024",
            quality="standard",
            n=1
        )
        # Pega a URL tempor√°ria gerada
        image_url = response.data[0].url

        # --- NOVA L√ìGICA ---
        # Baixa o conte√∫do da imagem a partir da URL
        st.info("üì• Baixando dados da imagem para armazenamento permanente...")
        image_response = requests.get(image_url)
        image_response.raise_for_status()  # Verifica se o download foi bem-sucedido

        # Converte os dados da imagem para Base64
        image_base64 = base64.b64encode(image_response.content).decode('utf-8')

        st.success("Imagem gerada e armazenada com sucesso!")

        # Retorna a string Base64, e n√£o mais a URL
        return f"data:image/png;base64,{image_base64}"

    except Exception as e:
        st.error(f"Ocorreu um erro ao gerar a imagem: {e}")
        return None

# <--- REMOVIDAS: As fun√ß√µes `classificar_categoria`, `detectar_tom_emocional`, e `detectar_tom_usuario` foram substitu√≠das pela nova `analisar_metadados_prompt`.

def detectar_idioma_com_ia(texto_usuario):
    """Usa a pr√≥pria OpenAI para detectar o idioma, um m√©todo mais preciso."""
    if not texto_usuario.strip():
        return 'pt'  # Retorna portugu√™s como padr√£o se o texto for vazio

    try:
        prompt = f"Qual o c√≥digo de idioma ISO 639-1 (ex: 'en', 'pt', 'es') do seguinte texto? Responda APENAS com o c√≥digo de duas letras.\n\nTexto: \"{texto_usuario}\""

        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5,  # Super curto e r√°pido
            temperature=0
        )
        idioma = resposta_modelo.choices[0].message.content.strip().lower()

        # Garante que a resposta tenha apenas 2 caracteres
        if len(idioma) == 2:
            return idioma
        else:
            return 'pt'  # Retorna um padr√£o seguro em caso de resposta inesperada

    except Exception as e:
        print(f"Erro ao detectar idioma com IA: {e}")
        return 'pt'  # Retorna um padr√£o seguro em caso de erro


def preparar_texto_para_fala(texto):
    """
    Prepara o texto para ser falado (removendo markdown, emojis, etc.).
    Esta fun√ß√£o √© usada pelo TTS (Text-to-Speech) do navegador, que n√£o depende de bibliotecas Python.
    """
    # Remove links mantendo apenas o texto vis√≠vel
    texto = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', texto)

    # Remove emojis
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    texto = emoji_pattern.sub('', texto)

    # Remove marca√ß√µes de markdown
    texto = texto.replace('**', '').replace('__', '')
    texto = texto.replace('*', '').replace('_', '')
    texto = texto.replace('~~', '').replace('```', '')

    # Remove cabe√ßalhos, cita√ß√µes e listas
    texto = re.sub(r'^\s*#+\s*', '', texto, flags=re.MULTILINE)
    texto = re.sub(r'^\s*>\s*', '', texto, flags=re.MULTILINE)
    texto = re.sub(r'^\s*[-*‚Ä¢]\s+', '', texto, flags=re.MULTILINE)

    # Substitui√ß√µes sutis
    texto = texto.replace(':', ',').replace('‚Äî', ',')
    texto = re.sub(r'(\d+)\.', r'\1,', texto)

    # Insere pausas claras ap√≥s pontua√ß√µes
    texto = re.sub(r'([.!?])\s*', r'\1 ... ', texto)

    # Pausas suaves em quebras de linha
    texto = texto.replace('\n', ' ... ')

    # Remove espa√ßos excessivos
    texto = re.sub(r'\s+', ' ', texto).strip()

    return texto

def carregar_memoria():
    try:
        with open("memoria_jarvis.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


def salvar_memoria(memoria):
    with open("memoria_jarvis.json", "w", encoding="utf-8") as f:
        json.dump(memoria, f, ensure_ascii=False, indent=4)

def adicionar_a_memoria(pergunta, resposta, modelo_emb):
    """
    Adiciona um novo par de pergunta e resposta √† mem√≥ria din√¢mica (vetores em sess√£o)
    e √† mem√≥ria persistente (arquivo JSON).
    """
    if not modelo_emb:
        st.error("O modelo de embedding n√£o est√° carregado. N√£o √© poss√≠vel salvar na mem√≥ria.")
        return

    try:
        # --- ETAPA 1: ATUALIZAR A MEM√ìRIA EM TEMPO DE EXECU√á√ÉO (SESSION_STATE) ---
        st.info("Atualizando a mem√≥ria din√¢mica da sess√£o...")

        novo_vetor = modelo_emb.encode([pergunta])

        if 'vetores_perguntas' in st.session_state and st.session_state.vetores_perguntas is not None:
            st.session_state.vetores_perguntas = np.vstack(
                [st.session_state.vetores_perguntas, novo_vetor])
        else:
            st.session_state.vetores_perguntas = novo_vetor

        nova_resposta_formatada = {'texto': resposta, 'tom': 'neutro'}
        if 'base_de_conhecimento' in st.session_state and st.session_state.base_de_conhecimento is not None:
             st.session_state.base_de_conhecimento['respostas'].append(
                [nova_resposta_formatada])
        else:
            st.session_state.base_de_conhecimento = {'respostas': [[nova_resposta_formatada]]}


        st.toast("‚úÖ Mem√≥ria din√¢mica atualizada para esta sess√£o!", icon="üß†")
        print(
            f"Novo tamanho da matriz de vetores na sess√£o: {st.session_state.vetores_perguntas.shape}")

        # --- ETAPA 2: PERSISTIR A MEM√ìRIA NO ARQUIVO JSON ---
        memoria_persistente = carregar_memoria()
        # <--- MODIFICADO: Usa a nova fun√ß√£o otimizada para obter a categoria
        categoria = analisar_metadados_prompt(pergunta).get('categoria', 'geral')

        nova_entrada = {
            "pergunta": pergunta,
            "respostas": [{"texto": resposta, "tom": "neutro"}]
        }

        if categoria not in memoria_persistente:
            memoria_persistente[categoria] = []

        if not any(item["pergunta"].lower() == pergunta.lower() for item in memoria_persistente[categoria]):
            memoria_persistente[categoria].append(nova_entrada)
            salvar_memoria(memoria_persistente)
            st.toast("üíæ Mem√≥ria tamb√©m salva permanentemente no arquivo JSON.", icon="üìù")
        else:
            st.toast("Essa pergunta j√° existe na mem√≥ria permanente.", icon="üí°")

    except Exception as e:
        st.error(f"Erro ao atualizar a mem√≥ria: {e}")

def processar_comando_lembrese(texto_do_comando):
    """Usa a OpenAI para extrair um t√≥pico e valor de um texto e salvar nas prefer√™ncias."""
    st.info("Processando informa√ß√£o para minha mem√≥ria de prefer√™ncias...")

    prompt = f"""
    Analise a seguinte afirma√ß√£o feita por um usu√°rio: '{texto_do_comando}'.
    Sua tarefa √© extrair o t√≥pico principal e o valor associado a ele.
    Responda apenas com um objeto JSON contendo as chaves "topico" e "valor".

    Exemplos:
    - Afirma√ß√£o: "meu time de futebol √© o Sport Club do Recife" -> Resposta: {{"topico": "time de futebol", "valor": "Sport Club do Recife"}}
    - Afirma√ß√£o: "meu anivers√°rio √© em 15 de maio" -> Resposta: {{"topico": "anivers√°rio", "valor": "15 de maio"}}
    - Afirma√ß√£o: "meu filme favorito √© Interestelar" -> Resposta: {{"topico": "filme favorito", "valor": "Interestelar"}}
    """

    try:
        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            response_format={"type": "json_object"}
        )

        resultado_json = json.loads(resposta_modelo.choices[0].message.content)
        topico = resultado_json.get("topico")
        valor = resultado_json.get("valor")

        if topico and valor:
            username = st.session_state.get("username", "default")
            preferencias = carregar_preferencias(username)
            preferencias[topico.lower()] = valor
            salvar_preferencias(preferencias, username)
            st.toast(
                f"Entendido! Guardei que seu '{topico}' √© '{valor}'.", icon="üëç")
        else:
            st.warning(
                "N√£o consegui identificar um t√≥pico e um valor claros para memorizar.")

    except Exception as e:
        st.error(f"Ocorreu um erro ao tentar memorizar a prefer√™ncia: {e}")


def carregar_chats(username):
    """Carrega os chats de um arquivo JSON no GitHub, descriptografando o conte√∫do."""
    if not username:
        return {}

    filename = f"dados/chats_historico_{username}.json"
    encrypted_file_content = carregar_dados_do_github(filename)

    if encrypted_file_content:
        try:
            decrypted_file_content = decrypt_file_content_general(
                encrypted_file_content)
            return json.loads(decrypted_file_content)
        except Exception as e:
            print(
                f"AVISO: Falha ao descriptografar chats de '{username}' do GitHub. Tentando como JSON bruto. Erro: {e}")
            try:
                return json.loads(encrypted_file_content)
            except json.JSONDecodeError:
                print(
                    f"ERRO FATAL: Conte√∫do do chat de '{username}' do GitHub n√£o √© um JSON v√°lido.")
                return {}
    return {}


def salvar_chats(username):
    """
    Salva os chats do usu√°rio no GitHub, ignorando objetos n√£o-serializ√°veis
    e criptografando o conte√∫do.
    """
    if not username or "chats" not in st.session_state:
        return

    chats_para_salvar = copy.deepcopy(st.session_state.chats)

    
    for chat_id, chat_data in chats_para_salvar.items():
        if "title" not in chat_data:
            chat_data["title"] = "Chat sem t√≠tulo"

    for chat_id, chat_data in chats_para_salvar.items():
        if "dataframe" in chat_data:
            del chat_data["dataframe"]
        if "messages" in chat_data:
            mensagens_serializaveis = [
                msg for msg in chat_data["messages"] if msg.get("type") != "plot"]
            chat_data["messages"] = mensagens_serializaveis

    data_json_string = json.dumps(
        chats_para_salvar, ensure_ascii=False, indent=4)
    encrypted_data_string = encrypt_file_content_general(data_json_string)

    filename = f"dados/chats_historico_{username}.json"
    mensagem_commit = f"Atualiza chat do usuario {username}"
    try:
        salvar_dados_no_github(
            filename, encrypted_data_string, mensagem_commit)
        print(f"Chats de '{username}' salvos no GitHub.")
    except Exception as e:
        print(f"Erro ao salvar chats no GitHub: {e}")



def escolher_resposta_por_contexto(entry):
    if entry["respostas"]:
        return random.choice(entry["respostas"])["texto"]
    return None


def buscar_resposta_local(pergunta_usuario, memoria, limiar=0.9):
    pergunta_usuario = pergunta_usuario.lower()
    melhor_match, melhor_score = None, 0
    for categoria in memoria:
        for item in memoria[categoria]:
            score = SequenceMatcher(
                None, pergunta_usuario, item["pergunta"].lower()).ratio()
            if score > melhor_score and score >= limiar:
                melhor_match, melhor_score = item, score
    if melhor_match:
        return escolher_resposta_por_contexto(melhor_match)
    return None

def adaptar_estilo_com_base_na_emocao(ultima_emocao):
    if not ultima_emocao:
        return ""
    if ultima_emocao in ["triste", "cansado", "preocupado"]:
        return "Use um tom acolhedor, calmo e emp√°tico. Responda com frases simples e positivas."
    elif ultima_emocao in ["feliz", "grato", "animado"]:
        return "Use um tom leve, entusiasmado e positivo. Reforce o bom humor do usu√°rio."
    elif ultima_emocao in ["irritado", "raivoso"]:
        return "Use um tom calmo, direto e respeitoso. Evite piadas ou ironia."
    elif ultima_emocao in ["ansioso", "confuso"]:
        return "Use um tom tranquilizador. D√™ respostas claras e objetivas para reduzir a ansiedade."
    else:
        return "Use um tom neutro e gentil."

def ia_fez_uma_pergunta(mensagem_ia):
    """
    Verifica se a √∫ltima mensagem da IA foi uma pergunta real para o usu√°rio.
    """
    mensagem_ia = mensagem_ia.strip().lower()

    if mensagem_ia.endswith("?"):
        return True
    perguntas_comuns = [
        "voc√™ gostaria", "deseja continuar", "posso ajudar em mais algo",
        "quer que eu", "gostaria que eu", "quer saber mais", "quer continuar",
        "prefere parar", "deseja que eu continue", "o que voc√™ acha",
    ]
    return any(p in mensagem_ia for p in perguntas_comuns)


# <--- MODIFICADO: Fun√ß√£o agora aceita `tom_do_usuario` para evitar uma chamada de API extra.
def responder_com_inteligencia(pergunta_usuario, modelo, historico_chat, memoria, resumo_contexto="", tom_do_usuario=None):
    """
    Decide como responder, com uma instru√ß√£o de idioma refor√ßada e precisa.
    """
    idioma_da_pergunta = detectar_idioma_com_ia(pergunta_usuario)
    instrucao_idioma_reforcada = f"Sua regra mais importante e inegoci√°vel √© responder estritamente no seguinte idioma: '{idioma_da_pergunta}'. N√£o use nenhum outro idioma sob nenhuma circunst√¢ncia."
    entrada_curta = len(pergunta_usuario.strip()) <= 3
    resposta_memoria = None

    if entrada_curta and "ultima_pergunta_ia" in st.session_state:
        ultima = st.session_state["ultima_pergunta_ia"]
        if ia_fez_uma_pergunta(ultima):
            pergunta_usuario = f"Minha resposta √©: '{pergunta_usuario}'. Com base na sua pergunta anterior: '{ultima}'"
        else:
            resposta_memoria = buscar_resposta_local(pergunta_usuario, memoria)
    else:
        resposta_memoria = buscar_resposta_local(pergunta_usuario, memoria)

    if modelo_embedding and st.session_state.get('vetores_perguntas') is not None:
        try:
            vetor_pergunta_usuario = modelo_embedding.encode([pergunta_usuario])
            scores_similaridade = cosine_similarity(
                vetor_pergunta_usuario, st.session_state.vetores_perguntas)
            indice_melhor_match = np.argmax(scores_similaridade)
            score_maximo = scores_similaridade[0, indice_melhor_match]
            LIMIAR_CONFIANCA = 0.8

            if score_maximo > LIMIAR_CONFIANCA:
                logging.info(
                    f"Resposta encontrada na mem√≥ria local com confian√ßa de {score_maximo:.2%}.")
                st.info(
                    f"Resposta encontrada na mem√≥ria local (Confian√ßa: {score_maximo:.2%}) üß†")
                respostas_possiveis = st.session_state.base_de_conhecimento[
                    'respostas'][indice_melhor_match]
                resposta_local = random.choice(respostas_possiveis)['texto']
                return {"texto": resposta_local, "origem": "local"}
        except Exception as e:
            logging.error(f"Erro ao processar com modelo local: {e}")
            st.warning(
                f"Erro ao processar com modelo local: {e}. Usando OpenAI.")

    username = st.session_state.get("username", "default")
    preferencias = carregar_preferencias(username)
    
    # <--- MODIFICADO: Usa o tom j√° detectado em vez de chamar a API de novo.
    if tom_do_usuario:
        st.sidebar.info(f"Tom detectado: {tom_do_usuario}")

    if precisa_buscar_na_web(pergunta_usuario):
        logging.info(
            f"Iniciando busca na web para a pergunta: '{pergunta_usuario}'")
        st.info("Buscando informa√ß√µes em tempo real na web... üåê")
        contexto_da_web = buscar_na_internet(pergunta_usuario)

        prompt_sistema = f"""{instrucao_idioma_reforcada}\n\nVoc√™ √© Jarvis, um assistente prestativo. Sua tarefa √© responder √† pergunta do usu√°rio de forma clara e direta, baseando-se ESTRITAMENTE nas informa√ß√µes de contexto da web.\n\nINFORMA√á√ïES SOBRE SEU USU√ÅRIO, ISRAEL: {json.dumps(preferencias, ensure_ascii=False)}\nO tom atual do usu√°rio parece ser: {tom_do_usuario}.\n\nContexto da Web:\n{contexto_da_web}"""
    else:
        logging.info("Pergunta n√£o requer busca na web, consultando a OpenAI.")
        st.info("üîç Pesquisando dados...")

        prompt_sistema = f"{instrucao_idioma_reforcada}\n\nVoc√™ √© Jarvis, um assistente prestativo."

        if tom_do_usuario:
            prompt_sistema += f"\nO tom do texto dele parece ser '{tom_do_usuario}'. Adapte seu estilo de resposta a isso."
        if preferencias_emocionais := carregar_emocoes(username):
            try: # Adicionado try-except para mais robustez
                ultima_emocao = list(preferencias_emocionais.values())[-1]
                if isinstance(ultima_emocao, dict):
                    ultima_emocao = ultima_emocao.get("emocao", "neutro")
                ajuste_de_estilo = adaptar_estilo_com_base_na_emocao(str(ultima_emocao))
                prompt_sistema += f"\nO usu√°rio parece estar se sentindo '{ultima_emocao}' recentemente. {ajuste_de_estilo}"
            except (IndexError, TypeError) as e:
                print(f"N√£o foi poss√≠vel obter a √∫ltima emo√ß√£o: {e}")


                username = st.session_state.get("username", "usu√°rio")
        if preferencias:
            prompt_sistema += f"\nLembre-se destas prefer√™ncias sobre seu usu√°rio, {username}: {json.dumps(preferencias, ensure_ascii=False)}"

        if resumo_contexto:
            prompt_sistema += f"\nLembre-se tamb√©m do contexto da conversa atual: {resumo_contexto}"

    mensagens_para_api = [{"role": "system", "content": prompt_sistema}]
    mensagens_para_api.extend(historico_chat)

    modelo_selecionado = st.session_state.get('admin_model_choice', 'gpt-4o')
    resposta_modelo = chamar_openai_com_retries(
        modelo_openai=modelo,
        mensagens=mensagens_para_api,
        modelo=modelo_selecionado
    )

    if resposta_modelo is None:
        return {
            "texto": "Desculpe, n√£o consegui obter resposta no momento. Tente novamente em instantes.",
            "origem": "erro_api"
        }

    resposta_ia = resposta_modelo.choices[0].message.content
    st.session_state["ultima_pergunta_ia"] = resposta_ia
    return {"texto": resposta_ia, "origem": "openai_web" if 'contexto_da_web' in locals() else 'openai'}


def analisar_imagem(image_file):
    try:
        st.info("Analisando a imagem com a IA...")
        image_bytes = image_file.getvalue()
        base64_image = base64.b64encode(image_bytes).decode('utf-8')
        image_type = image_file.type
        messages = [{"role": "user", "content": [{"type": "text", "text": "Descreva esta imagem em detalhes. Se for um diagrama ou texto, extraia as informa√ß√µes de forma estruturada."}, {
            "type": "image_url", "image_url": {"url": f"data:{image_type};base64,{base64_image}"}}]}]
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o", messages=messages, max_tokens=1024)
        st.success("An√°lise da imagem conclu√≠da!")
        return resposta_modelo.choices[0].message.content
    except Exception as e:
        st.error(f"Ocorreu um erro ao analisar a imagem: {e}")
        return "N√£o foi poss√≠vel analisar a imagem."

# <--- MODIFICADO: Fun√ß√£o agora aceita `metadados` para passar o tom do usu√°rio adiante.
def processar_entrada_usuario(prompt_usuario, metadados=None):
    chat_id = st.session_state.current_chat_id
    active_chat = st.session_state.chats[chat_id]
    active_chat = padronizar_chat(active_chat)
    st.session_state.chats[chat_id] = active_chat

    
    if metadados is None:
        metadados = {}

    df = active_chat.get("dataframe")
    if df is not None:
        resultado_analise = analisar_dados_com_ia(prompt_usuario, df)
        active_chat["messages"].append({
            "role": "assistant",
            "type": resultado_analise["type"],
            "content": resultado_analise["content"]
        })
        salvar_chats(st.session_state["username"])
        st.rerun()
        return

    historico_chat = [
        {"role": msg["role"], "content": msg["content"]}
        for msg in active_chat["messages"]
        if msg.get("type") == "text"
    ]

    ultima_emocao = st.session_state.get("ultima_emocao_usuario")
    if ultima_emocao:
        mensagem_sistema_emocional = {
            "role": "system",
            "content": f"Voc√™ √© um assistente prestativo. Sua principal diretriz √© ser sempre √∫til e positivo. O usu√°rio est√° se sentindo {ultima_emocao}. Leve isso em considera√ß√£o ao formular sua resposta, oferecendo apoio e uma perspectiva adequada √† emo√ß√£o atual dele, sem mencionar explicitamente a emo√ß√£o."
        }
        historico_chat.insert(0, mensagem_sistema_emocional)

    resumo_contexto = active_chat.get("resumo_curto_prazo", "")
    contexto_do_arquivo = active_chat.get("contexto_arquivo")

    if contexto_do_arquivo:
        tipos_de_codigo = ('.py', '.js', '.ts', '.html', '.htm', '.css', '.php', '.java', '.kt', '.c', '.cpp', '.h', '.cs', '.rb', '.go', '.swift', '.sql', '.json', '.xml', '.yaml', '.yml', '.md', '.sh', '.bat', '.ps1', '.R', '.pl', '.lua')
        nome_arquivo = active_chat.get("processed_file_name", "").lower()

        if nome_arquivo.endswith(tipos_de_codigo):
            # MODO PROGRAMA√á√ÉO (CORRETO)
            prompt_sistema_programacao = """
            Voc√™ √© Jarvis, um programador expert e assistente de c√≥digo s√™nior.
            Sua tarefa √© analisar, refatorar, depurar e criar c√≥digo com base no contexto fornecido e nas solicita√ß√µes do usu√°rio.
            Responda sempre de forma clara, fornecendo o c√≥digo em blocos formatados (ex: ```python ... ```) e explicando suas sugest√µes.
            Seja preciso, eficiente e siga as melhores pr√°ticas de programa√ß√£o.
            """
            historico_para_analise = [
                {"role": "system", "content": prompt_sistema_programacao},
                {"role": "user", "content": f"O seguinte arquivo de c√≥digo est√° em contexto para nossa conversa:\n`{nome_arquivo}`\n\nCONTE√öDO DO ARQUIVO:\n---\n{contexto_do_arquivo}\n---"},
                {"role": "assistant", "content": f"Entendido. O arquivo `{nome_arquivo}` foi carregado. Estou pronto para ajudar com o c√≥digo."}
            ]
        else:
            # MODO DOCUMENTO/DADOS (CORRETO)
            historico_para_analise = [
                {"role": "system", "content": "Voc√™ √© um assistente especialista em an√°lise de dados e documentos. Responda √†s perguntas do usu√°rio baseando-se ESTRITAMENTE no conte√∫do do documento fornecido abaixo."},
                {"role": "user", "content": f"CONTE√öDO DO DOCUMENTO PARA AN√ÅLISE:\n---\n{contexto_do_arquivo}\n---"},
                {"role": "assistant", "content": "Entendido. O conte√∫do do documento foi carregado. Estou pronto para responder suas perguntas sobre ele."}
            ]

        # Agora, esta linha usa o hist√≥rico que foi definido corretamente acima
        historico_para_analise.extend(historico_chat)
        historico_final = historico_para_analise
    else:
        historico_final = historico_chat



    # <--- MODIFICADO: Passa o tom do usu√°rio (dos metadados) para a fun√ß√£o de resposta.
    tom_do_usuario = metadados.get("sentimento_usuario")
    dict_resposta = responder_com_inteligencia(
        prompt_usuario, modelo, historico_final, memoria, resumo_contexto, tom_do_usuario=tom_do_usuario
    )

    active_chat["messages"].append({
        "role": "assistant",
        "type": "text",
        "content": dict_resposta["texto"],
        "origem": dict_resposta["origem"]
    })

    if len(active_chat["messages"]) == 2 and active_chat.get("title", "") == "Novo Chat":

        with st.spinner("Criando t√≠tulo para o chat..."):
            novo_titulo = gerar_titulo_conversa_com_ia(active_chat["messages"])
            active_chat["title"] = novo_titulo

    salvar_chats(st.session_state["username"])
    st.rerun()


def salvar_feedback(username, rating, comment):
    """Salva o feedback do usu√°rio em um arquivo JSON."""
    feedback_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "username": username,
        "rating": rating,
        "comment": comment
    }

    caminho_arquivo = "dados/feedback.json"
    os.makedirs("dados", exist_ok=True)

    dados_existentes = []
    if os.path.exists(caminho_arquivo) and os.path.getsize(caminho_arquivo) > 0:
        with open(caminho_arquivo, "r", encoding="utf-8") as f:
            try:
                dados_existentes = json.load(f)
            except json.JSONDecodeError:
                dados_existentes = []

    if not isinstance(dados_existentes, list):
        dados_existentes = []

    dados_existentes.append(feedback_data)

    with open(caminho_arquivo, "w", encoding="utf-8") as f:
        json.dump(dados_existentes, f, indent=4, ensure_ascii=False)


def gerar_resumo_curto_prazo(historico_chat):
    """Gera um resumo da conversa recente usando a OpenAI."""
    print("Gerando resumo de curto prazo...")
    ultimas_mensagens = historico_chat[-10:]
    conversa_para_resumir = "\n".join(
        [f"{msg['role']}: {msg['content']}" for msg in ultimas_mensagens])

    prompt = f"""
    A seguir est√° um trecho de uma conversa entre 'user' e 'assistant'. 
    Sua tarefa √© ler este trecho e resumi-lo em uma √∫nica e concisa frase em portugu√™s que capture o t√≥pico principal ou a √∫ltima informa√ß√£o relevante discutida.
    Este resumo ser√° usado como mem√≥ria de curto prazo para o assistente.

    Exemplo de resumo: "O usu√°rio estava perguntando sobre os detalhes de deploy de aplica√ß√µes Streamlit."

    Conversa:
    {conversa_para_resumir}

    Resumo conciso em uma frase:
    """

    try:
        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=100
        )
        resumo = resposta_modelo.choices[0].message.content.strip()
        return resumo
    except Exception as e:
        print(f"Erro ao gerar resumo: {e}")
        return ""


def gerar_titulo_conversa_com_ia(mensagens):
    """Usa a IA para criar um t√≠tulo curto para a conversa."""
    historico_para_titulo = [
        f"{msg['role']}: {msg['content']}" for msg in mensagens if msg.get('type') == 'text']
    conversa_inicial = "\n".join(historico_para_titulo)

    prompt = f"""
    Abaixo est√° o in√≠cio de uma conversa entre um usu√°rio e um assistente de IA. 
    Sua tarefa √© criar um t√≠tulo curto e conciso em portugu√™s (m√°ximo de 5 palavras) que resuma o t√≥pico principal da conversa.
    Responda APENAS com o t√≠tulo, sem nenhuma outra palavra ou pontua√ß√£o.

    CONVERSA:
    {conversa_inicial}

    T√çTULO CURTO:
    """

    try:
        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=15,
            temperature=0.2
        )
        titulo = resposta_modelo.choices[0].message.content.strip().replace(
            '"', '')
        return titulo if titulo else "Chat"
    except Exception as e:
        print(f"Erro ao gerar t√≠tulo: {e}")
        return "Chat"


def precisa_buscar_na_web(pergunta_usuario):
    """
    Usa a OpenAI para decidir rapidamente se uma pergunta requer busca na web.
    """
    if any(p in pergunta_usuario.lower() for p in ["link", "v√≠deo", "site", "inscri√ß√£o", "cadastro", "url"]):
        return True

    print("Verificando necessidade de busca na web...")
    prompt = f"""
    Analise a pergunta do usu√°rio e determine se ela requer informa√ß√µes em tempo real ou muito recentes para ser respondida com precis√£o.
    Responda apenas com a palavra 'BUSCA_WEB' se a busca for necess√°ria.
    Responda apenas com a palavra 'CHAT_PADRAO' se for uma pergunta geral, criativa, sobre a mem√≥ria interna ou que n√£o dependa do tempo.

    Exemplos que precisam de busca:
    - Qual a cota√ß√£o do d√≥lar hoje?
    - Quem ganhou o jogo do Sport ontem?
    - Quais as √∫ltimas not√≠cias sobre a OpenAI?
    - Como est√° o tempo em Recife?

    Exemplos que N√ÉO precisam de busca:
    - Quem descobriu o Brasil?
    - Me d√™ ideias para um prompt de imagem
    - Quem √© voc√™?
    - Crie uma lista de compras

    Pergunta do usu√°rio: "{pergunta_usuario}"
    """
    try:
        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        decisao = resposta_modelo.choices[0].message.content.strip().upper()
        print(f"Decis√£o do classificador: {decisao}")
        return "BUSCA_WEB" in decisao
    except Exception as e:
        print(f"Erro ao verificar necessidade de busca: {e}")
        return False


def buscar_na_internet(pergunta_usuario):
    """
    Pesquisa a pergunta na web usando a API Serper e retorna um resumo dos resultados com links.
    """
    print(f"Pesquisando na web por: {pergunta_usuario}")

    if not api_key_serper:
        return "ERRO: A chave da API Serper n√£o foi configurada."

    url = "[https://google.serper.dev/search](https://google.serper.dev/search)"
    payload = json.dumps({"q": pergunta_usuario, "gl": "br", "hl": "pt-br"})
    headers = {'X-API-KEY': api_key_serper, 'Content-Type': 'application/json'}

    try:
        response = requests.request("POST", url, headers=headers, data=payload)
        resultados = response.json().get('organic', [])

        if not resultados:
            return "Nenhum resultado encontrado na web."

        contexto_web = []
        for i, item in enumerate(resultados[:3]):
            titulo = item.get('title', 'Sem t√≠tulo')
            snippet = item.get('snippet', 'Sem descri√ß√£o')
            link = item.get('link', '#')
            contexto_web.append(
                f"üîπ **{titulo}**\n{snippet}\nüîó [Acessar site]({link})\n")

        return "\n\n".join(contexto_web)

    except Exception as e:
        return f"ERRO ao pesquisar na web: {e}"


def executar_analise_profunda(df):
    """Executa um conjunto de an√°lises de dados e retorna a sa√≠da como string."""
    buffer = io.StringIO()
    from contextlib import redirect_stdout
    with redirect_stdout(buffer):
        print("--- RESUMO ESTAT√çSTICO (NUM√âRICO) ---\n")
        print(df.describe(include=np.number)) # Mais expl√≠cito
        print("\n\n--- RESUMO CATEG√ìRICO ---\n")
        if not df.select_dtypes(include=['object', 'category']).empty:
            print(df.describe(include=['object', 'category']))
        else:
            print("Nenhuma coluna de texto (categ√≥rica) encontrada.")
        print("\n\n--- CONTAGEM DE VALORES √öNICOS ---\n")
        print(df.nunique())
        print("\n\n--- VERIFICA√á√ÉO DE DADOS FALTANTES (NULOS) ---\n")
        print(df.isnull().sum())
        print("\n\n--- MATRIZ DE CORRELA√á√ÉO (APENAS NUM√âRICO) ---\n")
        # numeric_only=True √© o padr√£o em vers√µes recentes do pandas, mas √© bom manter por compatibilidade.
        print(df.corr(numeric_only=True))
    return buffer.getvalue()


def analisar_dados_com_ia(prompt_usuario, df):
    """
    Usa a IA em um processo de duas etapas para analisar dados.
    """
    st.info("Gerando e executando an√°lise...")

    schema = df.head().to_string()

    prompt_gerador_codigo = f"""
Voc√™ √© um gerador de c√≥digo Python para an√°lise de dados com Pandas.
O usu√°rio tem um dataframe `df` com o seguinte schema:
{schema}

A pergunta do usu√°rio √©: "{prompt_usuario}"

Sua tarefa √© gerar um c√≥digo Python, e SOMENTE o c√≥digo, para obter os dados necess√°rios para responder √† pergunta.
- Use a fun√ß√£o `print()` para exibir todos os resultados brutos necess√°rios (tabelas, contagens, m√©dias, etc.).
- Se a pergunta pedir explicitamente um gr√°fico, use `plotly.express` e atribua a figura a uma vari√°vel chamada `fig`.
- **IMPORTANTE: Ao usar fun√ß√µes de agrega√ß√£o como `.mean()`, `.sum()`, ou `.corr()`, sempre inclua o argumento `numeric_only=True` para evitar erros com colunas de texto. Exemplo: `df.mean(numeric_only=True)`.**
- Responda apenas com o bloco de c√≥digo Python.
"""

    try:
        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo_codigo = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt_gerador_codigo}],
            temperature=0,
        )
        codigo_gerado = resposta_modelo_codigo.choices[0].message.content.strip()

        # Limpeza do c√≥digo gerado
        if codigo_gerado.startswith("```python"):
            codigo_gerado = codigo_gerado[9:].strip()
        elif codigo_gerado.startswith("```"):
            codigo_gerado = codigo_gerado[3:].strip()
        if codigo_gerado.endswith("```"):
            codigo_gerado = codigo_gerado[:-3].strip()

        local_vars = {"df": df, "pd": pd, "px": px}
        output_buffer = io.StringIO()

        from contextlib import redirect_stdout
        with redirect_stdout(output_buffer):
            exec(codigo_gerado, local_vars)

        if "fig" in local_vars:
            st.success("Gr√°fico gerado com sucesso!")
            return {"type": "plot", "content": local_vars["fig"]}

        resultados_brutos = output_buffer.getvalue().strip()

        if not resultados_brutos:
            return {"type": "text", "content": "A an√°lise foi executada, mas n√£o produziu resultados vis√≠veis."}

        st.info("An√°lise executada. Interpretando resultados para o usu√°rio...")

        prompt_interpretador = f"""
        Voc√™ √© Jarvis, um assistente de IA especialista em an√°lise de dados. Sua tarefa √© atuar como um analista de neg√≥cios e explicar os resultados de uma an√°lise de forma clara, visual e com insights para um usu√°rio final.
        A pergunta original do usu√°rio foi: "{prompt_usuario}"
        Abaixo est√£o os resultados brutos obtidos de um script Python:
        --- DADOS BRUTOS ---
        {resultados_brutos}
        --- FIM DOS DADOS BRUTOS ---

        Por favor, transforme esses dados brutos em um relat√≥rio amig√°vel.
        - **NUNCA** mostre as tabelas de dados brutos ou o texto t√©cnico.
        - Use Markdown, emojis (como üìä, üë§, üö®) e negrito para criar um "Dashboard de Insights R√°pidos".
        - Apresente os n√∫meros de forma clara (ex: "56,8%" em vez de "0.56788").
        - Identifique o principal "Insight Estrat√©gico" ou "Alerta" que os dados revelam.
        - No final, sugira 2 ou 3 perguntas inteligentes que o usu√°rio poderia fazer para aprofundar a an√°lise.
        """

        modelo_selecionado = st.session_state.get(
            'admin_model_choice', 'gpt-4o')
        resposta_modelo_interpretacao = modelo.chat.completions.create(
            model=modelo_selecionado,
            messages=[{"role": "user", "content": prompt_interpretador}],
            temperature=0.4,
        )

        resumo_claro = resposta_modelo_interpretacao.choices[0].message.content
        st.success("Relat√≥rio gerado!")
        return {"type": "text", "content": resumo_claro}

    except Exception as e:
        error_message = f"Desculpe, ocorreu um erro ao tentar analisar sua pergunta:\n\n**Erro:**\n`{e}`\n\n**C√≥digo que falhou:**\n```python\n{codigo_gerado if 'codigo_gerado' in locals() else 'N/A'}\n```"
        return {"type": "text", "content": error_message}


# --- INTERFACE GR√ÅFICA (STREAMLIT) ---
st.set_page_config(page_title="Jarvis IA", layout="wide")
st.markdown("""<style>.stApp { background-color: #0d1117; color: #c9d1d9; } .stTextInput, .stChatInput textarea { background-color: #161b22; color: #c9d1d9; border-radius: 8px; } .stButton button { background-color: #151b22; color: white; border-radius: 10px; border: none; }</style>""", unsafe_allow_html=True)

memoria = carregar_memoria()

# --- GEST√ÉO DE CHATS ---
def padronizar_chat(chat):
    return {
        "title": chat.get("title", "Novo Chat"),
        "messages": chat.get("messages", []),
        "contexto_arquivo": chat.get("contexto_arquivo"),
        "processed_file_name": chat.get("processed_file_name"),
        "dataframe": chat.get("dataframe"),
        "resumo_curto_prazo": chat.get("resumo_curto_prazo", ""),
        "ultima_mensagem_falada": chat.get("ultima_mensagem_falada")
    }


def create_new_chat():
    chat_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    st.session_state.chats[chat_id] = {
        "title": "Novo Chat",
        "messages": [],
        "contexto_arquivo": None,
        "processed_file_name": None,
        "dataframe": None,
        "resumo_curto_prazo": "",
        "ultima_mensagem_falada": None
    }
    st.session_state.current_chat_id = chat_id

    # st.rerun() n√£o √© necess√°rio aqui, pois o on_click do bot√£o j√° far√° isso.

def switch_chat(chat_id):
    st.session_state.current_chat_id = chat_id


def delete_chat(chat_id_to_delete):
    if chat_id_to_delete in st.session_state.chats:
        del st.session_state.chats[chat_id_to_delete]

        if st.session_state.current_chat_id == chat_id_to_delete:
            if st.session_state.chats:
                st.session_state.current_chat_id = list(
                    st.session_state.chats.keys())[-1]
            else:
                create_new_chat()

        salvar_chats(st.session_state["username"])
        # st.rerun() n√£o √© necess√°rio, o on_click do bot√£o far√° isso.

# <--- NOVO: Fun√ß√£o para o bot√£o de logout
def fazer_logout():
    """Limpa a sess√£o para deslogar o usu√°rio."""
    st.session_state.clear()


# --- INICIALIZA√á√ÉO E SIDEBAR ---
if "chats" not in st.session_state:
    raw_chats = carregar_chats(st.session_state["username"])
    st.session_state.chats = {
        chat_id: padronizar_chat(chat_data)
        for chat_id, chat_data in raw_chats.items()
    }

if not st.session_state.chats:
    create_new_chat()

if "current_chat_id" not in st.session_state or st.session_state.current_chat_id not in st.session_state.chats:
    st.session_state.current_chat_id = list(st.session_state.chats.keys())[-1]

chat_id = st.session_state.current_chat_id
active_chat = padronizar_chat(st.session_state.chats[chat_id])
st.session_state.chats[chat_id] = active_chat



def img_to_base64(path):
    with open(path, "rb") as f:
        data = f.read()
    return base64.b64encode(data).decode()


gif_b64 = img_to_base64("assets/jarvis-gif.gif")

with st.sidebar:
    st.markdown(
        f"""
        <div style="text-align: center;">
            <img src="data:image/gif;base64,{gif_b64}" width="150" style="display: inline-block;" />
            <h1 style="font-size: 28px; font-weight: bold; color: #00f0ff; margin-top: 10px;">
                ü§ñ Jarvis IA
            </h1>
        </div>
        """,
        unsafe_allow_html=True
    )

    st.sidebar.divider()
    st.sidebar.header("Painel do Usu√°rio")
    st.sidebar.page_link("pages/3_Gerenciar_Preferencias.py",
                         label="Minhas Prefer√™ncias", icon="‚öôÔ∏è")
    st.sidebar.page_link("pages/7_emocoes.py",
                         label="Gerenciar Emo√ß√µes", icon="üß†")
    st.sidebar.page_link("pages/4_Suporte_e_Ajuda.py",
                         label="Suporte e Ajuda", icon="üí°")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìö Ferramentas Externas")

    st.sidebar.markdown("""
    <style>
    a.link-button { display: block; background-color: #1f77b4; color: white !important; padding: 10px; text-align: center; border-radius: 8px; text-decoration: none; margin: 5px 0; font-weight: bold; transition: 0.3s; }
    a.link-button:hover { background-color: #005fa3; }
    </style>
    <a class='link-button' href='https://jarvis-lembrete.streamlit.app/' target='_blank'>üîî Jarvis Lembrete</a>
    <a class='link-button' href='https://jarvis-ia-video-analysis.streamlit.app/' target='_blank'>üé• Analisador de M√≠dia</a>
    """, unsafe_allow_html=True)

    if st.session_state.get("username") == ADMIN_USERNAME:
        st.sidebar.divider()
        st.sidebar.header("Painel do Admin")
        st.sidebar.page_link("pages/1_Gerenciar_Memoria.py",
                             label="Gerenciar Mem√≥ria", icon="üß†")
        st.sidebar.page_link("pages/2_Status_do_Sistema.py",
                             label="Status do Sistema", icon="üìä")
        st.sidebar.page_link("pages/5_Gerenciamento_de_Assinaturas.py",
                             label="Gerenciar Assinaturas", icon="üîë")
        st.sidebar.page_link("pages/6_Visualizar_Feedback.py",
                             label="Visualizar Feedback", icon="üìä")
        st.sidebar.radio(
            "Alternar Modelo OpenAI (Sess√£o Atual):",
            options=['gpt-4o', 'gpt-3.5-turbo'],
            key='admin_model_choice',
            help="Esta op√ß√£o afeta apenas a sua sess√£o de administrador e reseta ao sair. O padr√£o para todos os outros usu√°rios √© sempre gpt-4o."
        )

    st.sidebar.divider()

    # <--- MODIFICADO: Uso de on_click para estabilidade
    st.button("‚ûï Novo Chat", use_container_width=True, type="primary", on_click=create_new_chat)

    voz_ativada = st.checkbox(
        "üîä Ouvir respostas do Jarvis", value=False, key="voz_ativada")
    st.divider()

    st.write("#### Configura√ß√µes de Voz")
    idioma_selecionado = st.selectbox(
        "Idioma da Fala (Sa√≠da)",
        options=['pt-BR', 'en-US', 'es-ES', 'fr-FR', 'de-DE', 'it-IT'],
        index=0,
        key="idioma_fala",
        help="Escolha o idioma para o Jarvis falar suas respostas."
    )

with st.sidebar:
    st.write("#### Hist√≥rico de Chats")

    if "chats" in st.session_state:
        # Itera sobre uma c√≥pia para evitar problemas ao deletar
        for id, chat_data in reversed(list(st.session_state.chats.items())):
            chat_selected = (id == st.session_state.current_chat_id)
            col1, col2, col3 = st.columns([0.6, 0.2, 0.2])

            with col1:
                # CORRE√á√ÉO 1 (J√° feita por voc√™): Bot√£o de exibi√ß√£o
                st.button(chat_data.get("title", "Chat sem t√≠tulo"), key=f"chat_{id}",
                             use_container_width=True,
                             type="primary" if chat_selected else "secondary",
                             on_click=switch_chat,
                             args=(id,))
            with col2:
                with st.popover("‚úèÔ∏è", use_container_width=True):
                    # CORRE√á√ÉO 2: Campo de texto para renomear
                    new_title = st.text_input(
                        "Novo t√≠tulo:", value=chat_data.get("title", ""), key=f"rename_input_{id}")
                    if st.button("Salvar", key=f"save_rename_{id}"):
                        st.session_state.chats[id]["title"] = new_title
                        salvar_chats(st.session_state["username"])
                        st.rerun() 
            with col3:
                with st.popover("üóëÔ∏è", use_container_width=True):
                    # CORRE√á√ÉO 3: Mensagem de confirma√ß√£o de exclus√£o
                    st.write(
                        f"Tem certeza que deseja excluir **{chat_data.get('title', 'este chat')}**?")
                    
                    st.button("Sim, excluir!", type="primary", key=f"delete_confirm_{id}",
                                  on_click=delete_chat,
                                  args=(id,))

        st.button("üö™ Sair", use_container_width=True, type="secondary", on_click=fazer_logout)
    st.divider()

if st.session_state.get("show_feedback_form", False) and st.session_state.get("username") != ADMIN_USERNAME:
    with st.expander("‚≠ê Deixe seu Feedback", expanded=False):
        st.write("Sua opini√£o √© importante para a evolu√ß√£o do Jarvis!")

        with st.form("sidebar_feedback_form", clear_on_submit=True):
            rating = st.slider("Sua nota:", 1, 5, 3, key="feedback_rating")
            comment = st.text_area(
                "Coment√°rios (opcional):", key="feedback_comment")

            submitted = st.form_submit_button(
                "Enviar Feedback", use_container_width=True, type="primary")
            if submitted:
                salvar_feedback(st.session_state["username"], rating, comment)
                st.toast("Obrigado pelo seu feedback!", icon="üíñ")
                st.session_state["show_feedback_form"] = False
                st.rerun()

with st.expander("üìÇ Anexar Arquivos"):
    tipos_dados = ["csv", "xlsx", "xls", "json"]
    tipos_documentos = [
        "pdf", "docx", "txt", "py", "js", "ts", "html", "htm", "css",
        "php", "java", "kt", "c", "cpp", "h", "cs", "rb", "go",
        "swift", "sql", "xml", "yaml", "yml", "md", "sh", "bat", "ps1", "R", "pl", "lua"
    ]

    chat_id_for_key = st.session_state.current_chat_id

    arquivo = st.file_uploader(
        "üìÑ Documento, C√≥digo ou Dados (.csv, .xlsx, .json)",
        type=tipos_dados + tipos_documentos,
        key=f"uploader_doc_{chat_id_for_key}"
    )

    if arquivo and arquivo.name != active_chat.get("processed_file_name"):
        active_chat["contexto_arquivo"] = None
        active_chat["dataframe"] = None
        file_extension = arquivo.name.split('.')[-1].lower()

        if file_extension in tipos_dados:
            with st.spinner(f"Analisando '{arquivo.name}'..."):
                try:
                    df = None
                    if file_extension == 'csv':
                        df = pd.read_csv(arquivo)
                    elif file_extension in ['xlsx', 'xls']:
                        df = pd.read_excel(arquivo, engine='openpyxl')
                    elif file_extension == 'json':
                        df = pd.read_json(arquivo)

                    if df is not None:
                        active_chat["dataframe"] = df
                        active_chat["processed_file_name"] = arquivo.name
                        st.success(
                            f"Arquivo '{arquivo.name}' carregado! Jarvis est√° em modo de an√°lise.")
                        active_chat["messages"].append({
                            "role": "assistant", "type": "text",
                            "content": f"Arquivo `{arquivo.name}` carregado. Agora sou sua assistente de an√°lise de dados. Pe√ßa-me para gerar resumos, m√©dias, ou criar gr√°ficos."
                        })
                except Exception as e:
                    st.error(f"Erro ao carregar o arquivo de dados: {e}")
        else:
            active_chat["contexto_arquivo"] = extrair_texto_documento(arquivo)
            active_chat["processed_file_name"] = arquivo.name

        salvar_chats(st.session_state["username"])
        st.rerun()

    imagem = st.file_uploader(
        "üñºÔ∏è Imagem", type=["png", "jpg", "jpeg"], key=f"uploader_img_{chat_id_for_key}")
    if imagem and imagem.name != active_chat.get("processed_file_name"):
        st.image(imagem, width=200)
        active_chat["contexto_arquivo"] = analisar_imagem(imagem)
        active_chat["processed_file_name"] = imagem.name
        salvar_chats(st.session_state["username"])
        st.rerun()

    if active_chat.get("dataframe") is not None:
        st.info("Jarvis em 'Modo de An√°lise de Dados'.")
        with st.expander("Ver resumo dos dados"):
            st.dataframe(active_chat["dataframe"].head())
            buffer = io.StringIO()
            active_chat["dataframe"].info(buf=buffer)
            st.text(buffer.getvalue())
        # <--- MODIFICADO: Uso de on_click para estabilidade
        st.button("üóëÔ∏è Sair do Modo de An√°lise", type="primary", key=f"forget_btn_data_{chat_id}", on_click=create_new_chat)

    elif active_chat.get("contexto_arquivo"):
        st.info("Jarvis est√° em 'Modo de An√°lise de Documento'.")
        st.text_area("Conte√∫do extra√≠do:",
                     value=active_chat["contexto_arquivo"], height=200, key=f"contexto_arquivo_{chat_id}")
        # <--- MODIFICADO: Uso de on_click para estabilidade
        st.button("üóëÔ∏è Esquecer Arquivo Atual", type="primary", key=f"forget_btn_doc_{chat_id}", on_click=create_new_chat)


# --- √ÅREA PRINCIPAL DO CHAT ---
st.write(f"### {active_chat.get('title', 'Chat sem t√≠tulo')}")


for i, mensagem in enumerate(active_chat["messages"]):
    with st.chat_message(mensagem["role"]):
        if mensagem.get("type") == "plot":
            st.plotly_chart(mensagem["content"], use_container_width=True)
        elif mensagem.get("type") == "image":
            st.image(mensagem["content"], caption=mensagem.get("prompt", "Imagem gerada"))
            try:
                base64_data = mensagem["content"].split(",")[1]
                image_bytes = base64.b64decode(base64_data)
                st.download_button(
                    label="üì• Baixar Imagem",
                    data=image_bytes,
                    file_name="imagem_gerada_jarvis.png",
                    mime="image/png",
                    key=f"download_img_{i}"  
                )
            except Exception as e:
                st.warning(f"N√£o foi poss√≠vel gerar o bot√£o de download: {e}")
        else:
            st.write(mensagem["content"])

        if mensagem.get("origem") == "openai" and st.session_state.get("username") == ADMIN_USERNAME:
            if i > 0:
                pergunta_original = active_chat["messages"][i-1]["content"]
                resposta_original = mensagem["content"]
                cols = st.columns([1, 1, 10])

                with cols[0]:
                    if st.button("‚úÖ", key=f"save_{i}", help="Salvar resposta na mem√≥ria"):
                        adicionar_a_memoria(
                            pergunta_original, resposta_original, modelo_embedding)
                        mensagem["origem"] = "openai_curado"
                        salvar_chats(st.session_state["username"])
                        st.rerun()
                with cols[1]:
                    with st.popover("‚úèÔ∏è", help="Editar antes de salvar"):
                        with st.form(key=f"edit_form_{i}"):
                            st.write(
                                "Ajuste a pergunta e/ou a resposta antes de salvar.")
                            pergunta_editada = st.text_area(
                                "Pergunta:", value=pergunta_original, height=100)
                            resposta_editada = st.text_area(
                                "Resposta:", value=resposta_original, height=200)
                            if st.form_submit_button("Salvar Edi√ß√£o"):
                                adicionar_a_memoria(
                                    pergunta_editada, resposta_editada, modelo_embedding)
                                mensagem["origem"] = "openai_curado"
                                salvar_chats(st.session_state["username"])
                                st.rerun()

st.markdown("""
    <script>
        const chatContainer = window.parent.document.querySelector('.element-container:has(.stChatMessage)');
        if (chatContainer) { setTimeout(() => { chatContainer.scrollTop = chatContainer.scrollHeight; }, 100); }
    </script>
""", unsafe_allow_html=True)

if active_chat["messages"] and active_chat["messages"][-1]["role"] == "assistant" and voz_ativada:
    if active_chat["messages"][-1].get("type") == "text":
        resposta_ia = active_chat["messages"][-1]["content"]
        if resposta_ia != active_chat.get("ultima_mensagem_falada"):
            idioma_detectado = detectar_idioma_com_ia(resposta_ia)
            texto_limpo_para_fala = preparar_texto_para_fala(resposta_ia)
            resposta_formatada_para_voz = json.dumps(texto_limpo_para_fala)
            st.components.v1.html(f"""
            <script>
                function getVoices() {{ return new Promise(resolve => {{ let voices = speechSynthesis.getVoices(); if (voices.length) {{ resolve(voices); return; }} speechSynthesis.onvoiceschanged = () => {{ voices = speechSynthesis.getVoices(); resolve(voices); }}; }}); }}
                async function speak() {{ const text = {resposta_formatada_para_voz}; const idioma = '{idioma_detectado}'; if (!text || text.trim() === '') return; const allVoices = await getVoices(); let voicesForLang = allVoices.filter(v => v.lang.startsWith(idioma)); let desiredVoice; if (voicesForLang.length > 0) {{ if (idioma === 'pt') {{ const ptFemaleNames = ['Microsoft Francisca Online (Natural) - Portuguese (Brazil)', 'Microsoft Maria - Portuguese (Brazil)', 'Google portugu√™s do Brasil', 'Luciana', 'Joana']; for (const name of ptFemaleNames) {{ desiredVoice = voicesForLang.find(v => v.name === name); if (desiredVoice) break; }} }} if (!desiredVoice) {{ const femaleMarkers = ['Female', 'Feminino', 'Femme', 'Mujer']; desiredVoice = voicesForLang.find(v => femaleMarkers.some(marker => v.name.includes(marker))); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => v.default); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => !v.localService); }} if (!desiredVoice) {{ desiredVoice = voicesForLang[0]; }} }} const utterance = new SpeechSynthesisUtterance(text); if (desiredVoice) {{ utterance.voice = desiredVoice; utterance.lang = desiredVoice.lang; }} else {{ utterance.lang = idioma; }} utterance.pitch = 1.0; utterance.rate = 1.0; speechSynthesis.cancel(); speechSynthesis.speak(utterance); }}
                speak();
            </script>
            """, height=0)
            active_chat["ultima_mensagem_falada"] = resposta_ia
            salvar_chats(st.session_state["username"])


if 'pdf_para_download' in st.session_state:
    with st.chat_message("assistant"):
        st.download_button(
            label="üì• Baixar PDF",
            data=st.session_state['pdf_para_download'],
            file_name=st.session_state['pdf_filename'],
            mime="application/pdf",
            on_click=limpar_pdf_da_memoria
        )

# <--- NOVO: Fun√ß√£o para lidar com comandos, limpando o bloco principal
def processar_comandos(prompt_usuario, active_chat):
    """Processa comandos especiais como /imagine, /pdf, etc. Retorna True se um comando foi processado."""
    prompt_lower = prompt_usuario.lower().strip()

    if prompt_lower.startswith("/lembrese "):
        texto_para_lembrar = prompt_usuario[10:].strip()
        if texto_para_lembrar:
            with st.chat_message("assistant"):
                st.info("Memorizando sua prefer√™ncia...")
                processar_comando_lembrese(texto_para_lembrar)
        return True

    elif prompt_lower.startswith("/imagine "):
        prompt_da_imagem = prompt_usuario[9:].strip()
        if prompt_da_imagem:
            with st.chat_message("assistant"):
                dados_da_imagem = gerar_imagem_com_dalle(prompt_da_imagem)
                if dados_da_imagem:
                    active_chat["messages"].append(
                        {"role": "assistant", "type": "image", "content": dados_da_imagem, "prompt": prompt_da_imagem}
                    )
                    salvar_chats(st.session_state["username"])
            st.rerun()
        return True

    elif prompt_lower.startswith("/pdf "):
        topico_pdf = prompt_usuario[5:].strip()
        if topico_pdf:
            with st.chat_message("assistant"):
                with st.spinner("Criando seu PDF..."):
                    texto_completo_ia = gerar_conteudo_para_pdf(topico_pdf)
                    linhas_ia = texto_completo_ia.strip().split('\n')
                    titulo_documento = linhas_ia[0].replace('**', '').replace('###', '').replace('##', '').replace('#', '').strip()
                    texto_corpo = '\n'.join(linhas_ia[1:]).strip()
                    pdf_bytes = criar_pdf(texto_corpo, titulo_documento)
                    st.session_state['pdf_para_download'] = pdf_bytes
                    st.session_state['pdf_filename'] = f"{titulo_documento.replace(' ', '_')[:30]}.pdf"
            active_chat["messages"].append(
                {"role": "assistant", "type": "text", "content": f"Criei um PDF sobre '{titulo_documento}'. O bot√£o de download foi exibido."})
            salvar_chats(st.session_state["username"])
            st.rerun()
        return True
    
    elif prompt_lower.startswith("/gerar_codigo "):
        descricao_codigo = prompt_usuario[14:].strip()
        if descricao_codigo:
            with st.chat_message("assistant"):
                with st.spinner(f"Gerando c√≥digo para: '{descricao_codigo}'..."):
                    prompt_para_ia = f"""
                    Gere um script Python completo e funcional com base na seguinte descri√ß√£o.
                    O c√≥digo deve ser bem comentado, seguir as melhores pr√°ticas (PEP 8) e estar contido em um √∫nico bloco de c√≥digo.
                    Descri√ß√£o: "{descricao_codigo}"

                    Responda APENAS com o bloco de c√≥digo Python formatado em markdown.
                    Exemplo de Resposta:
                    ```python
                    # seu c√≥digo aqui
                    ```
                    """
                    modelo_selecionado = st.session_state.get('admin_model_choice', 'gpt-4o')
                    resposta_modelo = modelo.chat.completions.create(
                        model=modelo_selecionado,
                        messages=[{"role": "user", "content": prompt_para_ia}],
                        temperature=0.1,
                    )
                    codigo_gerado = resposta_modelo.choices[0].message.content
                    active_chat["messages"].append({"role": "assistant", "type": "text", "content": codigo_gerado})
                    salvar_chats(st.session_state["username"])
            st.rerun()
        return True

    elif prompt_lower.startswith(("/explicar", "/refatorar", "/depurar")):
        if not active_chat.get("contexto_arquivo"):
            st.warning("Para usar este comando, por favor, carregue um arquivo de c√≥digo primeiro.")
            active_chat["messages"].append({"role": "assistant", "type": "text", "content": "Para usar este comando, por favor, carregue um arquivo de c√≥digo primeiro."})
            # Opcional: Adicionar st.rerun() aqui para o aviso aparecer imediatamente
            # st.rerun() 
            return True 

        comando = prompt_lower.split(' ')[0] # /explicar, /refatorar, etc.
        instrucao_adicional = prompt_usuario[len(comando):].strip()
        nome_arquivo = active_chat.get("processed_file_name", "o arquivo atual")

        with st.chat_message("assistant"):
            with st.spinner(f"Processando '{comando}' no arquivo `{nome_arquivo}`..."):
                prompt_para_ia = f"""
                Voc√™ √© um programador expert. Sua tarefa √© executar a a√ß√£o '{comando}' no c√≥digo-fonte fornecido.

                Arquivo em an√°lise: `{nome_arquivo}`
                Instru√ß√µes adicionais do usu√°rio: "{instrucao_adicional if instrucao_adicional else 'Nenhuma'}"

                Responda de forma clara, com explica√ß√µes detalhadas e, se aplic√°vel, forne√ßa o bloco de c√≥digo modificado ou sugerido.

                C√ìDIGO-FONTE PARA AN√ÅLISE:
                ---
                {active_chat.get("contexto_arquivo")}
                ---
                """
                modelo_selecionado = st.session_state.get('admin_model_choice', 'gpt-4o')
                resposta_modelo = modelo.chat.completions.create(
                    model=modelo_selecionado,
                    messages=[{"role": "user", "content": prompt_para_ia}],
                    temperature=0.2,
                )
                resposta_analise = resposta_modelo.choices[0].message.content
                active_chat["messages"].append({"role": "assistant", "type": "text", "content": resposta_analise})
                salvar_chats(st.session_state["username"])
        st.rerun()
        return True

    
    elif prompt_lower == "/raiox":
        if active_chat.get("dataframe") is not None:
            df = active_chat.get("dataframe")
            with st.spinner("Executando Raio-X completo dos dados..."):
                resultados_brutos = executar_analise_profunda(df)
                prompt_interpretador = f"""Voc√™ √© Jarvis, um analista de dados s√™nior. O usu√°rio pediu um Raio-X completo do dataset. Abaixo est√£o os resultados brutos. Sua tarefa √© criar um relat√≥rio claro e com insights, explicando cada se√ß√£o (resumo estat√≠stico, categorias, valores √∫nicos, dados faltantes e correla√ß√µes) para o usu√°rio.\n\n--- DADOS BRUTOS ---\n{resultados_brutos}\n--- FIM DOS DADOS BRUTOS ---"""
                modelo_selecionado = st.session_state.get('admin_model_choice', 'gpt-4o')
                resposta_interpretada = modelo.chat.completions.create(
                    model=modelo_selecionado,
                    messages=[{"role": "user", "content": prompt_interpretador}]
                ).choices[0].message.content
                active_chat["messages"].append(
                    {"role": "assistant", "type": "text", "content": resposta_interpretada})
                salvar_chats(st.session_state["username"])
                st.rerun()
        else:
            st.warning("Para usar o comando /raiox, por favor, carregue um arquivo de dados primeiro.")
        return True
    
    return False


# --- ENTRADA DE TEXTO DO USU√ÅRIO (BLOCO REATORADO) ---
if prompt_usuario := st.chat_input("Fale com a Jarvis ou use /lembrese, /imagine, /pdf, /raiox, /gerar_codigo, /explicar, /refatorar, /depurar..."):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico imediatamente
    active_chat["messages"].append(
        {"role": "user", "type": "text", "content": prompt_usuario})
    salvar_chats(st.session_state["username"])

    # Tenta processar como um comando especial
    comando_foi_processado = processar_comandos(prompt_usuario, active_chat)

    # Se N√ÉO for um comando, processa como um chat normal
    if not comando_foi_processado:
        # 1. Analisa os metadados do prompt com UMA chamada de API otimizada
        metadados = analisar_metadados_prompt(prompt_usuario)
        
        # 2. Salva as emo√ß√µes e outros metadados coletados
        if st.session_state.username:
            timestamp_atual = datetime.now().isoformat()
            data_hora_obj = datetime.now()
            
            emocoes_dict[timestamp_atual] = {
                "emocao": metadados.get("emocao", "neutro"),
                "sentimento_mensagem_usuario": metadados.get("sentimento_usuario", "n/a"),
                "tipo_interacao": metadados.get("tipo_interacao", "conversa_geral"),
                "topico_interacao": metadados.get("categoria", "geral"),
                "dia_da_semana": data_hora_obj.strftime('%A').lower(),
                "periodo_do_dia": "manh√£" if 5 <= data_hora_obj.hour < 12 else "tarde" if 12 <= data_hora_obj.hour < 18 else "noite",
                "prompt_original": prompt_usuario
            }
            salvar_emocoes(emocoes_dict, st.session_state.username)
            # Atualiza a √∫ltima emo√ß√£o na sess√£o para uso imediato
            st.session_state["ultima_emocao_usuario"] = metadados.get("emocao", "neutro")

        # 3. Chama a fun√ß√£o de processamento de chat, passando os metadados j√° coletados
        processar_entrada_usuario(prompt_usuario, metadados=metadados)
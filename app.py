# ==============================================================================
# === 1. IMPORTA√á√ïES DE BIBLIOTECAS
# ==============================================================================
import logging
import streamlit as st
import copy
from openai import OpenAI
import json
from difflib import SequenceMatcher
import fitz  # PyMuPDF
import docx
import speech_recognition as sr
from dotenv import load_dotenv
import os
import datetime
import random
import re
import base64
import pandas as pd
import plotly.express as px
from fpdf import FPDF
from auth import check_password # Sua autentica√ß√£o local
from utils import carregar_preferencias, salvar_preferencias
import joblib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import requests
import librosa
import io
import soundfile
from fpdf.enums import XPos, YPos
# ==============================================================================
# === 2. VERIFICA√á√ÉO DE LOGIN E CONFIGURA√á√ÉO INICIAL
ADMIN_USERNAME = st.secrets.get("ADMIN_USERNAME", os.getenv("ADMIN_USERNAME"))
if not ADMIN_USERNAME:
    st.error("Nome de usu√°rio admin n√£o encontrado! Defina ADMIN_USERNAME em .env ou secrets.")
    st.stop()
# ==============================================================================

# Executa a verifica√ß√£o de login primeiro
if not check_password():
    st.stop()  # Interrompe a execu√ß√£o do script se o login falhar


# ==============================================================================
# === 3. CONEX√ÉO INTELIGENTE DE API (LOCAL E NUVEM)
# ==============================================================================

# Carrega as vari√°veis do arquivo .env (para o ambiente local)
load_dotenv()

# Verifica se a chave est√° nos "Secrets" do Streamlit (quando est√° na nuvem)
if "OPENAI_API_KEY" in st.secrets:
    # Ambiente da Nuvem
    st.sidebar.success("Jarvis Online", icon="‚òÅÔ∏è")
    api_key = st.secrets["OPENAI_API_KEY"]
    api_key_serper = st.secrets.get("SERPER_API_KEY") # Usamos .get() para n√£o dar erro se n√£o existir
else:
    # Ambiente Local
    st.sidebar.info("Jarvis Online", icon="‚òÅÔ∏è")
    api_key = os.getenv("OPENAI_API_KEY")
    api_key_serper = os.getenv("SERPER_API_KEY")

# Valida√ß√£o para garantir que a chave de API foi carregada
if not api_key:
    st.error("Chave de API da OpenAI n√£o encontrada! Verifique seu arquivo .env ou os Secrets na nuvem.")
    st.stop()

# Inicializa o modelo da OpenAI com a chave correta
modelo = OpenAI(api_key=api_key)


# ==============================================================================
# === 4. CONFIGURA√á√ÉO DE LOGS
# ==============================================================================

def setup_logging():
    """Configura o sistema de log para registrar eventos em um arquivo."""
    logging.basicConfig(
        filename='jarvis_log.txt',
        filemode='a', # 'a' para adicionar ao arquivo, 'w' para sobrescrever
        format='%(asctime)s - %(levelname)s - %(message)s',
        level=logging.INFO,
        encoding='utf-8'
    )

# Chame a fun√ß√£o uma vez no in√≠cio do script para configurar
setup_logging()


# ==============================================================================
# === 5. DEFINI√á√ÉO DAS FUN√á√ïES DO APLICATIVO
# ==============================================================================
# O resto do seu c√≥digo (a partir de @st.cache_resource) come√ßa aqui...
# Chame a fun√ß√£o uma vez no in√≠cio do script para configurar
setup_logging()

# --- CARREGAR O MODELO E FERRAMENTAS ---


@st.cache_resource
def carregar_modelos_locais():
    """
    Carrega os modelos locais apenas uma vez e guarda em cache.
    """
    print("Executando CARGA PESADA do c√©rebro local (isso s√≥ deve aparecer uma vez)...")
    try:
        modelo_emb = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2')
        vetores_p = np.load('vetores_perguntas_v2.npy')
        base_conhecimento = joblib.load('dados_conhecimento_v2.joblib')
        return modelo_emb, vetores_p, base_conhecimento
    except Exception as e:
        # Se der erro, retorna None para tudo
        return None, None, None


# --- CARREGAR O MODELO E FERRAMENTAS ---
modelo_embedding, vetores_perguntas, base_de_conhecimento = carregar_modelos_locais()

# Exibe a mensagem de status no painel lateral
if modelo_embedding:
    st.sidebar.success("Mem√≥ria ativada.", icon="üíæ")
else:
    st.sidebar.error("Arquivos do modelo local n√£o encontrados.")

# --- Fun√ß√µes do Aplicativo ---

def limpar_pdf_da_memoria():
    """Remove os dados do PDF do st.session_state para o bot√£o de download desaparecer."""
    if 'pdf_para_download' in st.session_state:
        del st.session_state['pdf_para_download']
    if 'pdf_filename' in st.session_state:
        del st.session_state['pdf_filename']

def gerar_conteudo_para_pdf(topico):
    """Usa a IA para gerar um texto bem formatado sobre um t√≥pico para o PDF."""
    prompt = f"Por favor, escreva um texto detalhado e bem estruturado sobre o seguinte t√≥pico para ser inclu√≠do em um documento PDF. Organize com par√°grafos claros e, se apropriado, use listas. T√≥pico: '{topico}'"
    resposta_modelo = modelo.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2048
    )
    return resposta_modelo.choices[0].message.content


def criar_pdf(texto_corpo, titulo_documento):
    """
    Cria um arquivo PDF em mem√≥ria, interpretando formata√ß√£o Markdown
    e usando uma fonte Unicode local de forma robusta e portatil.
    """
    pdf = FPDF()
    pdf.add_page()

    # Constr√≥i o caminho completo e robusto para as fontes
    script_dir = os.path.dirname(__file__)
    font_path_regular = os.path.join(script_dir, 'assets', 'DejaVuSans.ttf')
    font_path_bold = os.path.join(script_dir, 'assets', 'DejaVuSans-Bold.ttf')

    try:
        pdf.add_font('DejaVu', '', font_path_regular)
        pdf.add_font('DejaVu', 'B', font_path_bold)
        FONT_FAMILY = 'DejaVu'
    except FileNotFoundError:
        print("AVISO: Arquivos de fonte n√£o encontrados. Verifique a pasta 'assets'. Usando Helvetica.")
        FONT_FAMILY = 'Helvetica'

    # O resto da fun√ß√£o continua exatamente igual...
    pdf.set_font(FONT_FAMILY, 'B', 18)
    pdf.multi_cell(0, 10, titulo_documento, new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='C')
    pdf.ln(15)

    texto_corpo_ajustado = texto_corpo.strip().replace('*\n', '* ').replace('-\n', '- ')
    linhas = texto_corpo_ajustado.split('\n')

    for linha in linhas:
        linha = linha.strip()
        if linha.startswith('### '):
            pdf.set_font(FONT_FAMILY, 'B', 14)
            pdf.multi_cell(0, 8, linha.lstrip('### ').strip(), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(4)
        elif linha.startswith('## '):
            pdf.set_font(FONT_FAMILY, 'B', 16)
            pdf.multi_cell(0, 10, linha.lstrip('## ').strip(), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(6)
        elif linha.startswith('# '):
            pdf.set_font(FONT_FAMILY, 'B', 18)
            pdf.multi_cell(0, 12, linha.lstrip('# ').strip(), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(8)
        elif linha.startswith('**') and linha.endswith('**'):
            pdf.set_font(FONT_FAMILY, 'B', 12)
            texto_negrito = linha.strip('**')
            pdf.multi_cell(0, 7, texto_negrito, new_x=XPos.LMARGIN, new_y=YPos.NEXT)
            pdf.ln(3)
        elif linha.startswith('* ') or linha.startswith('- '):
            pdf.set_font(FONT_FAMILY, '', 12)
            bullet = "‚Ä¢" if FONT_FAMILY == 'DejaVu' else "*"
            pdf.cell(8, 7, f"  {bullet} ")
            texto_da_linha = linha.lstrip('* ').lstrip('- ').strip()
            write_with_mixed_styles(texto_da_linha, pdf, FONT_FAMILY)
            pdf.ln(2)
        elif linha:
            pdf.set_font(FONT_FAMILY, '', 12)
            write_with_mixed_styles(linha, pdf, FONT_FAMILY)
            pdf.ln(5)

    return bytes(pdf.output())

def write_with_mixed_styles(text, pdf, font_family):
    """
    Escreve uma linha de texto no PDF, alternando entre estilos normal e negrito
    com base na marca√ß√£o '**'.
    """
    parts = text.split('**')
    for i, part in enumerate(parts):
        if i % 2 == 1:
            pdf.set_font(font_family, 'B')
        else:
            pdf.set_font(font_family, '')
        pdf.write(7, part)
    pdf.ln()

def extrair_texto_documento(uploaded_file):
    """Extrai o texto de arquivos PDF, DOCX, TXT, Excel, e v√°rias linguagens de programa√ß√£o e scripts de banco de dados."""
    nome_arquivo = uploaded_file.name

    if nome_arquivo.endswith(('.xlsx', '.xls')):
        try:
            df = pd.read_excel(uploaded_file, engine='openpyxl')
            return df.to_csv(index=False)
        except Exception as e:
            return f"Erro ao ler o arquivo Excel: {e}"
    elif nome_arquivo.endswith(".pdf"):
        texto = ""
        with fitz.open(stream=uploaded_file.read(), filetype="pdf") as doc:
            for page in doc:
                texto += page.get_text()
        return texto
    elif nome_arquivo.endswith(".docx"):
        doc = docx.Document(uploaded_file)
        return "\n".join([p.text for p in doc.paragraphs])
    elif nome_arquivo.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")
    # --- Modifica√ß√£o ABRANGENTE para arquivos de programa√ß√£o e script ---
    elif nome_arquivo.endswith((
        '.py',      # Python
        '.js',      # JavaScript
        '.ts',      # TypeScript
        '.html',    # HTML
        '.htm',     # HTML
        '.css',     # CSS
        '.php',     # PHP
        '.java',    # Java
        '.kt',      # Kotlin
        '.c',       # C
        '.cpp',     # C++
        '.h',       # C/C++ Header
        '.cs',      # C#
        '.rb',      # Ruby
        '.go',      # Go
        '.swift',   # Swift
        '.sql',     # SQL (for MySQL, PostgreSQL, etc.)
        '.json',    # JSON
        '.xml',     # XML
        '.yaml',    # YAML
        '.yml',     # YAML
        '.md',      # Markdown
        '.sh',      # Shell Script
        '.bat',     # Batch Script
        '.ps1',     # PowerShell Script
        '.R',       # R
        '.pl',      # Perl
        '.lua'      # Lua
        # Adicione mais extens√µes conforme necess√°rio
    )):
        try:
            # Tenta UTF-8 primeiro, que √© o mais comum para c√≥digo
            return uploaded_file.read().decode("utf-8")
        except UnicodeDecodeError:
            # Se UTF-8 falhar, tenta Latin-1 (ou outra codifica√ß√£o comum no seu contexto)
            return uploaded_file.read().decode("latin-1")
        except Exception as e:
            return f"Erro ao ler o arquivo de c√≥digo/script: {e}"
    # --- FIM da modifica√ß√£o ---
    return "Formato de arquivo n√£o suportado."


def gerar_imagem_com_dalle(prompt_para_imagem):
    try:
        st.info(
            f"üé® Gerando imagem com DALL-E 3 para: '{prompt_para_imagem}'...")
        response = modelo.images.generate(
            model="dall-e-3", prompt=prompt_para_imagem, size="1024x1024", quality="standard", n=1)
        image_url = response.data[0].url
        st.success("Imagem gerada com sucesso!")
        return image_url
    except Exception as e:
        st.error(f"Ocorreu um erro ao gerar a imagem: {e}")
        return None


def classificar_categoria(pergunta):
    prompt = f"Classifique esta pergunta em uma √∫nica categoria simples (como geografia, hist√≥ria, sentimentos, programa√ß√£o, etc):\nPergunta: {pergunta}"
    resposta = modelo.chat.completions.create(
        model="gpt-4o", messages=[{"role": "user", "content": prompt}])
    return resposta.choices[0].message.content.strip().lower()


def detectar_tom_emocional(resposta):
    prompt = f"Qual o tom emocional desta resposta? Use uma s√≥ palavra: neutro, feliz, triste, sens√≠vel, etc.\nResposta: {resposta}"
    resposta_api = modelo.chat.completions.create(
        model="gpt-4o", messages=[{"role": "user", "content": prompt}])
    return resposta_api.choices[0].message.content.strip().lower()

def detectar_tom_usuario(pergunta_usuario):
    """Detecta o tom emocional da pergunta do usu√°rio."""
    prompt = f"""
    Analise o texto do usu√°rio abaixo e resuma o tom emocional ou o estado de esp√≠rito dele em poucas palavras (ex: 'apressado', 'curioso', 'frustrado', 'descontra√≠do', 'formal').
    Responda apenas com a descri√ß√£o do tom.

    Texto do usu√°rio: "{pergunta_usuario}"
    """
    try:
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=20
        )
        return resposta_modelo.choices[0].message.content.strip()
    except Exception as e:
        print(f"Erro ao detectar tom do usu√°rio: {e}")
        return "" # Retorna vazio em caso de erro


# [SUBSTITUA a antiga fun√ß√£o detectar_idioma POR ESTA]

def detectar_idioma_com_ia(texto_usuario):
    """Usa a pr√≥pria OpenAI para detectar o idioma, um m√©todo mais preciso."""
    if not texto_usuario.strip():
        return 'pt' # Retorna portugu√™s como padr√£o se o texto for vazio

    try:
        prompt = f"Qual o c√≥digo de idioma ISO 639-1 (ex: 'en', 'pt', 'es') do seguinte texto? Responda APENAS com o c√≥digo de duas letras.\n\nTexto: \"{texto_usuario}\""
        
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5, # Super curto e r√°pido
            temperature=0
        )
        idioma = resposta_modelo.choices[0].message.content.strip().lower()
        
        # Garante que a resposta tenha apenas 2 caracteres
        if len(idioma) == 2:
            return idioma
        else:
            return 'pt' # Retorna um padr√£o seguro em caso de resposta inesperada
            
    except Exception as e:
        print(f"Erro ao detectar idioma com IA: {e}")
        return 'pt' # Retorna um padr√£o seguro em caso de erro


def preparar_texto_para_fala(texto):
    # Remove links mantendo apenas o texto vis√≠vel
    texto = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', texto)

    # Remove emojis
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    texto = emoji_pattern.sub('', texto)

    # Remove marca√ß√µes de markdown
    texto = texto.replace('**', '').replace('__', '')
    texto = texto.replace('*', '').replace('_', '')
    texto = texto.replace('~~', '').replace('```', '')

    # Remove cabe√ßalhos, cita√ß√µes e listas
    texto = re.sub(r'^\s*#+\s*', '', texto, flags=re.MULTILINE)
    texto = re.sub(r'^\s*>\s*', '', texto, flags=re.MULTILINE)
    texto = re.sub(r'^\s*[-*‚Ä¢]\s+', '', texto, flags=re.MULTILINE)

    # Substitui√ß√µes sutis
    texto = texto.replace(':', ',').replace('‚Äî', ',')
    texto = re.sub(r'(\d+)\.', r'\1,', texto)

    # Insere pausas claras ap√≥s pontua√ß√µes
    texto = re.sub(r'([.!?])\s*', r'\1 ... ', texto)

    # Pausas suaves em quebras de linha
    texto = texto.replace('\n', ' ... ')

    # Remove espa√ßos excessivos
    texto = re.sub(r'\s+', ' ', texto).strip()

    return texto

def extrair_features(data, sample_rate):
    """
    Extrai 110 features de dados de √°udio para ser compat√≠vel com o modelo treinado.
    """
    # MFCC (40) -> 40 mean + 40 std = 80 features
    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)
    # Chroma (12) -> 12 mean + 12 std = 24 features
    chroma = librosa.feature.chroma_stft(y=data, sr=sample_rate)
    # ZCR -> 1 mean + 1 std = 2 features
    zcr = librosa.feature.zero_crossing_rate(data)
    # RMS -> 1 mean + 1 std = 2 features
    rms = librosa.feature.rms(y=data)
    # Spectral Centroid -> 1 mean + 1 std = 2 features
    centroid = librosa.feature.spectral_centroid(y=data, sr=sample_rate)
    
    # Total: 80 + 24 + 2 + 2 + 2 = 110 features
    features = np.hstack([
        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),
        np.mean(chroma, axis=1), np.std(chroma, axis=1),
        np.mean(zcr), np.std(zcr),
        np.mean(rms), np.std(rms),
        np.mean(centroid), np.std(centroid)
    ])
    return features

def analisar_tom_de_voz(audio_data_wav):
    """
    Analisa os dados de √°udio WAV para detectar uma emo√ß√£o usando um modelo pr√©-treinado.
    """
    try:
        # Carrega o modelo pr√©-treinado do arquivo
        modelo = joblib.load("modelo_emocoes_voz.joblib")

        # Converte os dados de √°udio em um formato que o librosa possa ler
        data, sample_rate = soundfile.read(io.BytesIO(audio_data_wav))
        
        # Extrai as caracter√≠sticas do √°udio
        features = extrair_features(data, sample_rate)
        
        # Usa o modelo para prever a emo√ß√£o
        # O .reshape(1, -1) √© necess√°rio para formatar os dados para o modelo
        resultado = modelo.predict(features.reshape(1, -1))
        
        # Retorna a emo√ß√£o prevista (ex: 'feliz', 'triste', 'neutro')
        return resultado[0]

    except FileNotFoundError:
        print("AVISO: Arquivo 'modelo_emocoes_voz.joblib' n√£o encontrado. An√°lise de tom de voz desativada.")
        return "neutro" # Retorna neutro se o modelo n√£o for encontrado
    except Exception as e:
        print(f"Erro na an√°lise de tom de voz real: {e}")
        return "neutro" # Retorna um valor seguro em caso de outro erro

def carregar_memoria():
    try:
        with open("memoria_jarvis.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


def salvar_memoria(memoria):
    with open("memoria_jarvis.json", "w", encoding="utf-8") as f:
        json.dump(memoria, f, ensure_ascii=False, indent=4)



def processar_comando_lembrese(texto_do_comando):
    """Usa a OpenAI para extrair um t√≥pico e valor de um texto e salvar nas prefer√™ncias."""
    st.info("Processando informa√ß√£o para minha mem√≥ria de prefer√™ncias...")

    prompt = f"""
    Analise a seguinte afirma√ß√£o feita por um usu√°rio: '{texto_do_comando}'.
    Sua tarefa √© extrair o t√≥pico principal e o valor associado a ele.
    Responda apenas com um objeto JSON contendo as chaves "topico" e "valor".

    Exemplos:
    - Afirma√ß√£o: "meu time de futebol √© o Sport Club do Recife" -> Resposta: {{"topico": "time de futebol", "valor": "Sport Club do Recife"}}
    - Afirma√ß√£o: "meu anivers√°rio √© em 15 de maio" -> Resposta: {{"topico": "anivers√°rio", "valor": "15 de maio"}}
    - Afirma√ß√£o: "meu filme favorito √© Interestelar" -> Resposta: {{"topico": "filme favorito", "valor": "Interestelar"}}
    """

    try:
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            response_format={"type": "json_object"}
        )

        resultado_json = json.loads(resposta_modelo.choices[0].message.content)
        topico = resultado_json.get("topico")
        valor = resultado_json.get("valor")

        if topico and valor:
            username = st.session_state.get("username", "default")
            preferencias = carregar_preferencias(username)
            preferencias[topico.lower()] = valor
            salvar_preferencias(preferencias, username)
            st.toast(
                f"Entendido! Guardei que seu '{topico}' √© '{valor}'.", icon="üëç")
        else:
            st.warning(
                "N√£o consegui identificar um t√≥pico e um valor claros para memorizar.")

    except Exception as e:
        st.error(f"Ocorreu um erro ao tentar memorizar a prefer√™ncia: {e}")


# NOVO carregar_chats
def carregar_chats(username):
    """Carrega os chats de um arquivo JSON espec√≠fico do usu√°rio."""
    if not username:
        return {} # Retorna um dicion√°rio vazio se n√£o houver nome de usu√°rio

    filename = f"chats_historico_{username}.json"
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {} # Se o arquivo do usu√°rio n√£o existir, retorna um hist√≥rico vazio.


# NOVO salvar_chats
def salvar_chats(username):
    """
    Salva os chats do usu√°rio, ignorando objetos n√£o-serializ√°veis como 
    DataFrames (no n√≠vel do chat) e Figuras Plotly (no n√≠vel das mensagens).
    """
    if not username or "chats" not in st.session_state:
        return

    # 1. Cria uma c√≥pia exata e segura do hist√≥rico de chats
    chats_para_salvar = copy.deepcopy(st.session_state.chats)

    # 2. Itera sobre cada chat na C√ìPIA
    for chat_id, chat_data in chats_para_salvar.items():
        
        # 2a. Remove o DataFrame do n√≠vel do chat, se existir
        if "dataframe" in chat_data:
            del chat_data["dataframe"]

        # 2b. (NOVA L√ìGICA) Filtra a lista de mensagens para remover as que n√£o s√£o serializ√°veis
        if "messages" in chat_data:
            mensagens_serializaveis = []
            for msg in chat_data["messages"]:
                # Adiciona a mensagem √† nova lista apenas se o tipo N√ÉO for 'plot'
                if msg.get("type") != "plot":
                    mensagens_serializaveis.append(msg)
            
            # Substitui a lista de mensagens antiga pela nova, j√° filtrada
            chat_data["messages"] = mensagens_serializaveis

    # 3. Salva a c√≥pia totalmente limpa no arquivo JSON
    filename = f"chats_historico_{username}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(chats_para_salvar, f, ensure_ascii=False, indent=4)


def escolher_resposta_por_contexto(entry):
    if entry["respostas"]:
        return random.choice(entry["respostas"])["texto"]
    return None


def buscar_resposta_local(pergunta_usuario, memoria, limiar=0.9):
    pergunta_usuario = pergunta_usuario.lower()
    melhor_match, melhor_score = None, 0
    for categoria in memoria:
        for item in memoria[categoria]:
            score = SequenceMatcher(
                None, pergunta_usuario, item["pergunta"].lower()).ratio()
            if score > melhor_score and score >= limiar:
                melhor_match, melhor_score = item, score
    if melhor_match:
        return escolher_resposta_por_contexto(melhor_match)
    return None


# [SUBSTITUA SUA FUN√á√ÉO responder_com_inteligencia POR ESTA VERS√ÉO APRIMORADA]

# [VERS√ÉO DE DEBUG da fun√ß√£o responder_com_inteligencia]

# [VERS√ÉO FINAL da fun√ß√£o responder_com_inteligencia]

def responder_com_inteligencia(pergunta_usuario, modelo, historico_chat, resumo_contexto="", tom_de_voz_detectado=None):
    """
    Decide como responder, com uma instru√ß√£o de idioma refor√ßada e precisa.
    """
    # --- ETAPA 0: DETEC√á√ÉO DE IDIOMA PRECISA COM IA ---
    idioma_da_pergunta = detectar_idioma_com_ia(pergunta_usuario)
    instrucao_idioma_reforcada = f"Sua regra mais importante e inegoci√°vel √© responder estritamente no seguinte idioma: '{idioma_da_pergunta}'. N√£o use nenhum outro idioma sob nenhuma circunst√¢ncia."

    # --- ETAPA 1: Tenta responder com a mem√≥ria local primeiro ---
    if modelo_embedding:
        try:
            vetor_pergunta_usuario = modelo_embedding.encode([pergunta_usuario])
            scores_similaridade = cosine_similarity(vetor_pergunta_usuario, vetores_perguntas)
            indice_melhor_match = np.argmax(scores_similaridade)
            score_maximo = scores_similaridade[0, indice_melhor_match]
            LIMIAR_CONFIANCA = 0.8

            if score_maximo > LIMIAR_CONFIANCA:
                logging.info(f"Resposta encontrada na mem√≥ria local com confian√ßa de {score_maximo:.2%}.")
                st.info(f"Resposta encontrada na mem√≥ria local (Confian√ßa: {score_maximo:.2%}) üß†")
                respostas_possiveis = base_de_conhecimento['respostas'][indice_melhor_match]
                resposta_local = random.choice(respostas_possiveis)['texto']
                return {"texto": resposta_local, "origem": "local"}
            
        except Exception as e:
            logging.error(f"Erro ao processar com modelo local: {e}")
            st.warning(f"Erro ao processar com modelo local: {e}. Usando OpenAI.")
    
    # Carrega as prefer√™ncias do usu√°rio e detecta o tom
    username = st.session_state.get("username", "default")
    preferencias = carregar_preferencias(username)
    tom_do_usuario = detectar_tom_usuario(pergunta_usuario)
    if tom_do_usuario:
        st.sidebar.info(f"Tom detectado: {tom_do_usuario}")

    # --- ETAPA 2: Decide se precisa de informa√ß√µes da internet ---
    if precisa_buscar_na_web(pergunta_usuario):
        logging.info(f"Iniciando busca na web para a pergunta: '{pergunta_usuario}'")
        st.info("Buscando informa√ß√µes em tempo real na web... üåê")
        contexto_da_web = buscar_na_internet(pergunta_usuario)
        
        prompt_sistema = f"""{instrucao_idioma_reforcada}\n\nVoc√™ √© Jarvis, um assistente prestativo. Sua tarefa √© responder √† pergunta do usu√°rio de forma clara e direta, baseando-se ESTRITAMENTE nas informa√ß√µes de contexto da web.\n\nINFORMA√á√ïES SOBRE SEU USU√ÅRIO, ISRAEL: {json.dumps(preferencias, ensure_ascii=False)}\nO tom atual do usu√°rio parece ser: {tom_do_usuario}.\n\nContexto da Web:\n{contexto_da_web}"""
    else:
        # --- ETAPA 3: Se n√£o precisa de busca, usa o fluxo de chat padr√£o ---
        logging.info("Pergunta n√£o requer busca na web, consultando a OpenAI.")
        st.info("Consultando a OpenAI...")
        
        prompt_sistema = f"{instrucao_idioma_reforcada}\n\nVoc√™ √© Jarvis, um assistente prestativo."
        
        if tom_de_voz_detectado and tom_de_voz_detectado != "neutro":
            prompt_sistema += f"\nO tom de voz do usu√°rio parece ser '{tom_de_voz_detectado}'. Adapte sua resposta a isso."
        if tom_do_usuario:
            prompt_sistema += f"\nO tom do texto dele parece ser '{tom_do_usuario}'. Adapte seu estilo de resposta a isso."
        if preferencias:
            prompt_sistema += f"\nLembre-se destas prefer√™ncias sobre seu usu√°rio, Israel: {json.dumps(preferencias, ensure_ascii=False)}"
        if resumo_contexto:
            prompt_sistema += f"\nLembre-se tamb√©m do contexto da conversa atual: {resumo_contexto}"
    
    mensagens_para_api = [{"role": "system", "content": prompt_sistema}]
    mensagens_para_api.extend(historico_chat)

    # Chamada final para a OpenAI
    resposta_modelo = modelo.chat.completions.create(
        messages=mensagens_para_api,
        model="gpt-4o"
    )
    resposta_ia = resposta_modelo.choices[0].message.content
    return {"texto": resposta_ia, "origem": "openai_web" if 'contexto_da_web' in locals() else 'openai'}


def analisar_imagem(image_file):
    try:
        st.info("Analisando a imagem com a IA...")
        image_bytes = image_file.getvalue()
        base64_image = base64.b64encode(image_bytes).decode('utf-8')
        image_type = image_file.type
        messages = [{"role": "user", "content": [{"type": "text", "text": "Descreva esta imagem em detalhes. Se for um diagrama ou texto, extraia as informa√ß√µes de forma estruturada."}, {
            "type": "image_url", "image_url": {"url": f"data:{image_type};base64,{base64_image}"}}]}]
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o", messages=messages, max_tokens=1024)
        st.success("An√°lise da imagem conclu√≠da!")
        return resposta_modelo.choices[0].message.content
    except Exception as e:
        st.error(f"Ocorreu um erro ao analisar a imagem: {e}")
        return "N√£o foi poss√≠vel analisar a imagem."


def escutar_audio():
    idioma_para_reconhecimento = st.session_state.get("idioma_fala", "pt-BR")
    recognizer = sr.Recognizer()
    try:
        with sr.Microphone() as source:
            st.info(f"Fale agora (em {idioma_para_reconhecimento})...")
            recognizer.adjust_for_ambient_noise(source)
            audio_capturado = recognizer.listen(source)

        st.info("Processando √°udio...")

        # Etapa 1: Transcrever o texto
        texto_reconhecido = recognizer.recognize_google(
            audio_capturado, language=idioma_para_reconhecimento)
        st.success("√Åudio transcrito!")

        # Etapa 2: Analisar o tom de voz
        tom_de_voz = analisar_tom_de_voz(audio_capturado.get_wav_data())
        st.info(f"Tom de voz detectado (exemplo): {tom_de_voz}")

        # Etapa 3: Retornar AMBOS os resultados
        return texto_reconhecido, tom_de_voz

    except sr.UnknownValueError:
        st.warning("N√£o consegui entender o que voc√™ disse.")
        # Retorna DOIS valores em caso de erro
        return None, None
    except sr.RequestError as e:
        st.error(f"N√£o foi poss√≠vel se conectar ao servi√ßo de reconhecimento; {e}")
        # Retorna DOIS valores em caso de erro
        return None, None
    except Exception as e:
        st.error(f"Ocorreu um erro ao acessar o microfone: {e}")
        print(f"ERRO DETALHADO DO MICROFONE: {e}")
        # Retorna DOIS valores em caso de erro
        return None, None


# [SUBSTITUA SUA FUN√á√ÉO processar_entrada_usuario POR ESTA]

def processar_entrada_usuario(prompt_usuario, tom_voz=None):
    chat_id = st.session_state.current_chat_id
    active_chat = st.session_state.chats[chat_id]

    # --- MODO DE AN√ÅLISE DE DADOS ---
    if active_chat.get("dataframe") is not None:
        df = active_chat.get("dataframe")
        
        if prompt_usuario.lower() in ["/sair", "/exit", "/sair_analise"]:
            active_chat["dataframe"] = None
            active_chat["processed_file_name"] = None
            active_chat["messages"].append({
                "role": "assistant", "type": "text", 
                "content": "Modo de an√°lise desativado. Como posso ajudar?"
            })
            salvar_chats(st.session_state["username"])
            st.rerun()
            return

        resultado_analise = analisar_dados_com_ia(prompt_usuario, df)
        active_chat["messages"].append({
            "role": "assistant",
            "type": resultado_analise["type"],
            "content": resultado_analise["content"]
        })
        salvar_chats(st.session_state["username"])
        st.rerun()
        return

    # --- MODO DE CHAT NORMAL (L√ìGICA COMPLETA) ---
    historico_chat = [
        {"role": msg["role"], "content": msg["content"]}
        for msg in active_chat["messages"]
        if msg.get("type") == "text"
    ]

    numero_de_mensagens = len(historico_chat)
    if numero_de_mensagens > 0 and numero_de_mensagens % 6 == 0:
        resumo_atualizado = gerar_resumo_curto_prazo(historico_chat)
        active_chat["resumo_curto_prazo"] = resumo_atualizado
        st.toast("üß† Mem√≥ria de curto prazo atualizada.", icon="üîÑ")

    resumo_contexto = active_chat.get("resumo_curto_prazo", "")
    contexto_do_arquivo = active_chat.get("contexto_arquivo")

    if contexto_do_arquivo:
        historico_para_analise = [
            {"role": "system", "content": "Voc√™ √© um assistente especialista em an√°lise de dados e documentos. Responda √†s perguntas do usu√°rio baseando-se ESTRITAMENTE no conte√∫do do documento fornecido abaixo."},
            {"role": "user", "content": f"CONTE√öDO DO DOCUMENTO PARA AN√ÅLISE:\n---\n{contexto_do_arquivo}\n---"},
            {"role": "assistant", "content": "Entendido. O conte√∫do do documento foi carregado. Estou pronto para responder suas perguntas sobre ele."}
        ]
        historico_para_analise.extend(historico_chat)
        historico_final = historico_para_analise
    else:
        historico_final = historico_chat

    dict_resposta = responder_com_inteligencia(
        prompt_usuario, modelo, historico_final, resumo_contexto, tom_de_voz_detectado=tom_voz)

    active_chat["messages"].append({
        "role": "assistant",
        "type": "text",
        "content": dict_resposta["texto"],
        "origem": dict_resposta["origem"]
    })
    salvar_chats(st.session_state["username"])
    st.rerun()

   

def adicionar_a_memoria(pergunta, resposta):
    """Adiciona um novo par de pergunta/resposta √† mem√≥ria local."""
    try:
        memoria_atual = carregar_memoria()
        # Usa a fun√ß√£o que j√° temos para classificar a categoria
        categoria = classificar_categoria(pergunta)

        nova_entrada = {
            "pergunta": pergunta,
            "respostas": [{"texto": resposta, "tom": "neutro"}]
        }

        if categoria not in memoria_atual:
            memoria_atual[categoria] = []

        # Evita adicionar duplicatas exatas
        if not any(item["pergunta"].lower() == pergunta.lower() for item in memoria_atual[categoria]):
            memoria_atual[categoria].append(nova_entrada)
            salvar_memoria(memoria_atual)
            st.toast("‚úÖ Mem√≥ria atualizada com sucesso!", icon="üß†")
        else:
            st.toast("Essa pergunta j√° existe na mem√≥ria.", icon="üí°")

    except Exception as e:
        st.error(f"Erro ao salvar na mem√≥ria: {e}")


def gerar_resumo_curto_prazo(historico_chat):
    """Gera um resumo da conversa recente usando a OpenAI."""
    print("Gerando resumo de curto prazo...")

    # Pega as √∫ltimas 10 mensagens para n√£o sobrecarregar o prompt
    ultimas_mensagens = historico_chat[-10:]
    conversa_para_resumir = "\n".join(
        [f"{msg['role']}: {msg['content']}" for msg in ultimas_mensagens])

    prompt = f"""
    A seguir est√° um trecho de uma conversa entre 'user' e 'assistant'. 
    Sua tarefa √© ler este trecho e resumi-lo em uma √∫nica e concisa frase em portugu√™s que capture o t√≥pico principal ou a √∫ltima informa√ß√£o relevante discutida.
    Este resumo ser√° usado como mem√≥ria de curto prazo para o assistente.

    Exemplo de resumo: "O usu√°rio estava perguntando sobre os detalhes de deploy de aplica√ß√µes Streamlit."

    Conversa:
    {conversa_para_resumir}

    Resumo conciso em uma frase:
    """

    try:
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=100
        )
        resumo = resposta_modelo.choices[0].message.content.strip()
        return resumo
    except Exception as e:
        print(f"Erro ao gerar resumo: {e}")
        return ""  # Retorna vazio em caso de erro

# NOVA FUN√á√ÉO 1: O "DETECTOR DE ATUALIDADES"


def precisa_buscar_na_web(pergunta_usuario):
    """
    Usa a OpenAI para decidir rapidamente se uma pergunta requer busca na web.
    """
    print("Verificando necessidade de busca na web...")
    prompt = f"""
    Analise a pergunta do usu√°rio e determine se ela requer informa√ß√µes em tempo real ou muito recentes para ser respondida com precis√£o.
    Responda apenas com a palavra 'BUSCA_WEB' se a busca for necess√°ria.
    Responda apenas com a palavra 'CHAT_PADRAO' se for uma pergunta geral, criativa, sobre a mem√≥ria interna ou que n√£o dependa do tempo.

    Exemplos que precisam de busca:
    - Qual a cota√ß√£o do d√≥lar hoje?
    - Quem ganhou o jogo do Sport ontem?
    - Quais as √∫ltimas not√≠cias sobre a OpenAI?
    - Como est√° o tempo em Recife?

    Exemplos que N√ÉO precisam de busca:
    - Quem descobriu o Brasil?
    - Me d√™ ideias para um prompt de imagem
    - Quem √© voc√™?
    - Crie uma lista de compras

    Pergunta do usu√°rio: "{pergunta_usuario}"
    """
    try:
        resposta_modelo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        decisao = resposta_modelo.choices[0].message.content.strip().upper()
        print(f"Decis√£o do classificador: {decisao}")
        return "BUSCA_WEB" in decisao
    except Exception as e:
        print(f"Erro ao verificar necessidade de busca: {e}")
        return False

# FERRAMENTA DE BUSCA
def buscar_na_internet(pergunta_usuario):
    """
    Pesquisa a pergunta na web usando a API Serper e retorna um resumo dos resultados.
    """
    print(f"Pesquisando na web por: {pergunta_usuario}")
    
    #api_key_serper = st.secrets["SERPER_API_KEY"]
    if not api_key_serper:
        return "ERRO: A chave da API Serper n√£o foi configurada."

    url = "https://google.serper.dev/search"
    payload = json.dumps({"q": pergunta_usuario, "gl": "br", "hl": "pt-br"})
    headers = {'X-API-KEY': api_key_serper, 'Content-Type': 'application/json'}

    try:
        response = requests.request("POST", url, headers=headers, data=payload)
        resultados = response.json().get('organic', [])

        if not resultados:
            return "Nenhum resultado encontrado na web."

        # Pega os 3 principais resultados para montar o contexto
        contexto_web = []
        for i, item in enumerate(resultados[:3]):
            titulo = item.get('title', 'Sem t√≠tulo')
            snippet = item.get('snippet', 'Sem descri√ß√£o')
            contexto_web.append(f"Fonte {i+1} ({titulo}): {snippet}")

        return "\n".join(contexto_web)
    except Exception as e:
        return f"ERRO ao pesquisar na web: {e}"


# [SUBSTITUA SUA FUN√á√ÉO analisar_dados_com_ia POR ESTA VERS√ÉO APRIMORADA]
def analisar_dados_com_ia(prompt_usuario, df):
    """
    Usa a IA em um processo de duas etapas:
    1. Gera e executa c√≥digo Python para obter resultados brutos.
    2. Envia os resultados brutos para a IA novamente para gerar uma interpreta√ß√£o amig√°vel.
    """
    st.info("Gerando e executando an√°lise...")

    # --- ETAPA 1: Gerar o c√≥digo Python de an√°lise ---
    schema = df.head().to_string()
    
    # A linha abaixo e todo o bloco de texto foram indentados corretamente
    prompt_gerador_codigo = f"""
Voc√™ √© um gerador de c√≥digo Python para an√°lise de dados com Pandas.
O usu√°rio tem um dataframe `df` com o seguinte schema:
{schema}

A pergunta do usu√°rio √©: "{prompt_usuario}"

Sua tarefa √© gerar um c√≥digo Python, e SOMENTE o c√≥digo, para obter os dados necess√°rios para responder √† pergunta.
- Use a fun√ß√£o `print()` para exibir todos os resultados brutos necess√°rios (tabelas, contagens, m√©dias, etc.).
- Se a pergunta pedir explicitamente um gr√°fico, use `plotly.express` e atribua a figura a uma vari√°vel chamada `fig`.
- **IMPORTANTE: Ao usar fun√ß√µes de agrega√ß√£o como `.mean()`, `.sum()`, ou `.corr()`, sempre inclua o argumento `numeric_only=True` para evitar erros com colunas de texto. Exemplo: `df.mean(numeric_only=True)`.**
- Responda apenas com o bloco de c√≥digo Python.
"""

    try:
        resposta_modelo_codigo = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt_gerador_codigo}],
            temperature=0,
        )
        codigo_gerado = resposta_modelo_codigo.choices[0].message.content.strip()

        if codigo_gerado.startswith("```python"):
            codigo_gerado = codigo_gerado[9:].strip()
        elif codigo_gerado.startswith("```"):
            codigo_gerado = codigo_gerado[3:].strip()
        if codigo_gerado.endswith("```"):
            codigo_gerado = codigo_gerado[:-3].strip()

        # --- ETAPA 2: Executar o c√≥digo e capturar a sa√≠da bruta ---
        local_vars = {"df": df, "pd": pd, "px": px}
        output_buffer = io.StringIO()
        
        from contextlib import redirect_stdout
        with redirect_stdout(output_buffer):
            exec(codigo_gerado, local_vars)

        # Se um gr√°fico foi gerado, retorne-o imediatamente.
        if "fig" in local_vars:
            st.success("Gr√°fico gerado com sucesso!")
            return {"type": "plot", "content": local_vars["fig"]}

        resultados_brutos = output_buffer.getvalue().strip()
        
        if not resultados_brutos:
            return {"type": "text", "content": "A an√°lise foi executada, mas n√£o produziu resultados vis√≠veis."}
        
        st.info("An√°lise executada. Interpretando resultados para o usu√°rio...")

        # --- ETAPA 3 (NOVA): Enviar a sa√≠da bruta para a IA para interpreta√ß√£o ---
        prompt_interpretador = f"""
        Voc√™ √© Jarvis, um assistente de IA especialista em an√°lise de dados. Sua tarefa √© atuar como um analista de neg√≥cios e explicar os resultados de uma an√°lise de forma clara, visual e com insights para um usu√°rio final.

        A pergunta original do usu√°rio foi: "{prompt_usuario}"

        Abaixo est√£o os resultados brutos obtidos de um script Python:
        --- DADOS BRUTOS ---
        {resultados_brutos}
        --- FIM DOS DADOS BRUTOS ---

        Por favor, transforme esses dados brutos em um relat√≥rio amig√°vel.
        - **NUNCA** mostre as tabelas de dados brutos ou o texto t√©cnico.
        - Use Markdown, emojis (como üìä, üë§, üö®) e negrito para criar um "Dashboard de Insights R√°pidos".
        - Apresente os n√∫meros de forma clara (ex: "56,8%" em vez de "0.56788").
        - Identifique o principal "Insight Estrat√©gico" ou "Alerta" que os dados revelam.
        - No final, sugira 2 ou 3 perguntas inteligentes que o usu√°rio poderia fazer para aprofundar a an√°lise.
        """
        
        resposta_modelo_interpretacao = modelo.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt_interpretador}],
            temperature=0.4,
        )
        
        resumo_claro = resposta_modelo_interpretacao.choices[0].message.content

        st.success("Relat√≥rio gerado!")
        return {"type": "text", "content": resumo_claro}

    except Exception as e:
        error_message = f"Desculpe, ocorreu um erro ao tentar analisar sua pergunta:\n\n**Erro:**\n`{e}`\n\n**C√≥digo que falhou:**\n```python\n{codigo_gerado}\n```"
        return {"type": "text", "content": error_message}
# --- INTERFACE GR√ÅFICA (STREAMLIT) ---
st.set_page_config(page_title="Jarvis IA", layout="wide")
st.markdown("""<style>.stApp { background-color: #0d1117; color: #c9d1d9; } .stTextInput, .stChatInput textarea { background-color: #161b22; color: #c9d1d9; border-radius: 8px; } .stButton button { background-color: #151b22; color: white; border-radius: 10px; border: none; }</style>""", unsafe_allow_html=True)

memoria = carregar_memoria()

# --- GEST√ÉO DE CHATS ---


def create_new_chat():
    """Cria um novo chat com todos os campos necess√°rios."""
    chat_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    st.session_state.chats[chat_id] = {
        "title": "Jarvis IA - Welcome!",
        "messages": [],
        "contexto_arquivo": None,
        "processed_file_name": None,
        "dataframe": None,  # Importante para consist√™ncia
        "resumo_curto_prazo": "",
        "ultima_mensagem_falada": None
    }
    st.session_state.current_chat_id = chat_id
    return chat_id


def switch_chat(chat_id):
    st.session_state.current_chat_id = chat_id


def delete_chat(chat_id_to_delete):
    if chat_id_to_delete in st.session_state.chats:
        del st.session_state.chats[chat_id_to_delete]
        
        # Se o chat deletado era o chat atual, mude para outro chat ou crie um novo
        if st.session_state.current_chat_id == chat_id_to_delete:
            if st.session_state.chats: # Se ainda existirem outros chats
                st.session_state.current_chat_id = list(st.session_state.chats.keys())[-1]
            else: # Se n√£o houver mais chats, crie um novo
                create_new_chat()
        
        # Garante que o estado atualizado seja salvo no arquivo
        salvar_chats(st.session_state["username"])
        
        st.rerun() # Recarrega a aplica√ß√£o para refletir a mudan√ßa


# --- INICIALIZA√á√ÉO E SIDEBAR ---
if "chats" not in st.session_state:
    st.session_state.chats = carregar_chats(st.session_state["username"])
    if not st.session_state.chats:
        create_new_chat()
    if "current_chat_id" not in st.session_state or st.session_state.current_chat_id not in st.session_state.chats:
        st.session_state.current_chat_id = list(
            st.session_state.chats.keys())[-1]

chat_id = st.session_state.current_chat_id
active_chat = st.session_state.chats[chat_id]

with st.sidebar:
    st.write("### ü§ñ Jarvis IA")

    st.sidebar.title("Navega√ß√£o")

    st.sidebar.page_link("app.py", label="Chat Principal", icon="ü§ñ")
    
    st.sidebar.divider()
    st.sidebar.header("Painel do Usu√°rio")
    st.sidebar.page_link("pages/3_Gerenciar_Preferencias.py", label="Minhas Prefer√™ncias", icon="‚öôÔ∏è")

    #if st.session_state.get("username") == ADMIN_USERNAME:
        #st.sidebar.divider()
        #st.sidebar.header("Painel do Admin")
        #st.sidebar.page_link("pages/1_Gerenciar_Memoria.py", label="Gerenciar Mem√≥ria", icon="üß†")
        #st.sidebar.page_link("pages/2_Status_do_Sistema.py", label="Status do Sistema", icon="üìä")
    
    st.sidebar.divider()
    
    if st.button("‚ûï Novo Chat", use_container_width=True, type="primary"):
        create_new_chat()
        st.rerun()

    voz_ativada = st.checkbox(
        "üîä Ouvir respostas do Jarvis", value=False, key="voz_ativada")
    st.divider()

    st.write("#### Configura√ß√µes de Voz")
    idioma_selecionado = st.selectbox(
        "Idioma da Fala (Entrada)",
        options=['pt-BR', 'en-US', 'es-ES', 'fr-FR', 'de-DE', 'it-IT'],
        index=0,
        key="idioma_fala",
        help="Escolha o idioma que voc√™ ir√° falar no microfone."
    )

    st.write("#### Hist√≥rico de Chats")
    if "chats" in st.session_state:
        for id, chat_data in reversed(list(st.session_state.chats.items())):
            col1, col2, col3 = st.columns([0.7, 0.15, 0.15])
            with col1:
                if st.button(chat_data["title"], key=f"chat_{id}", use_container_width=True, type="secondary" if id != st.session_state.current_chat_id else "primary"):
                    switch_chat(id)
                    st.rerun()
            with col2:
                with st.popover("‚úèÔ∏è", use_container_width=True):
                    new_title = st.text_input(
                        "Novo t√≠tulo:", value=chat_data["title"], key=f"rename_input_{id}")
                    if st.button("Salvar", key=f"save_rename_{id}"):
                        st.session_state.chats[id]["title"] = new_title
                        salvar_chats(st.session_state["username"])
                        st.rerun()
            with col3:
                with st.popover("üóëÔ∏è", use_container_width=True):
                    st.write(
                        f"Tem certeza que deseja excluir '{chat_data['title']}'?")
                    if st.button("Sim, excluir!", type="primary", key=f"delete_confirm_{id}"):
                        delete_chat(id)
    st.divider()

    with st.expander("üìÇ Anexar Arquivos"):
        tipos_dados = ["csv", "xlsx", "xls", "json"]
        tipos_documentos = [
            "pdf", "docx", "txt", "py", "js", "ts", "html", "htm", "css", 
            "php", "java", "kt", "c", "cpp", "h", "cs", "rb", "go", 
            "swift", "sql", "xml", "yaml", "yml", "md", "sh", "bat", "ps1", "R", "pl", "lua"
        ]
        
        chat_id_for_key = st.session_state.current_chat_id
        
        arquivo = st.file_uploader(
            "üìÑ Documento, C√≥digo ou Dados (.csv, .xlsx, .json)",
            type=tipos_dados + tipos_documentos,
            key=f"uploader_doc_{chat_id_for_key}"
        )

        if arquivo and arquivo.name != active_chat.get("processed_file_name"):
            active_chat["contexto_arquivo"] = None
            active_chat["dataframe"] = None
            file_extension = arquivo.name.split('.')[-1].lower()

            if file_extension in tipos_dados:
                with st.spinner(f"Analisando '{arquivo.name}'..."):
                    try:
                        df = None
                        if file_extension == 'csv': df = pd.read_csv(arquivo)
                        elif file_extension in ['xlsx', 'xls']: df = pd.read_excel(arquivo, engine='openpyxl')
                        elif file_extension == 'json': df = pd.read_json(arquivo)
                        
                        if df is not None:
                            active_chat["dataframe"] = df
                            active_chat["processed_file_name"] = arquivo.name
                            st.success(f"Arquivo '{arquivo.name}' carregado! Jarvis est√° em modo de an√°lise.")
                            active_chat["messages"].append({
                                "role": "assistant", "type": "text", 
                                "content": f"Arquivo `{arquivo.name}` carregado. Agora sou seu assistente de an√°lise de dados. Pe√ßa-me para gerar resumos, m√©dias, ou criar gr√°ficos."
                            })
                    except Exception as e:
                        st.error(f"Erro ao carregar o arquivo de dados: {e}")
            else:
                active_chat["contexto_arquivo"] = extrair_texto_documento(arquivo)
                active_chat["processed_file_name"] = arquivo.name
            
            salvar_chats(st.session_state["username"])
            st.rerun()

        imagem = st.file_uploader(
            "üñºÔ∏è Imagem", type=["png", "jpg", "jpeg"], key=f"uploader_img_{chat_id_for_key}")
        if imagem and imagem.name != active_chat.get("processed_file_name"):
            st.image(imagem, width=200)
            active_chat["contexto_arquivo"] = analisar_imagem(imagem)
            active_chat["processed_file_name"] = imagem.name
            salvar_chats(st.session_state["username"])
            st.rerun()
        
        if active_chat.get("dataframe") is not None:
            st.info("Jarvis em 'Modo de An√°lise de Dados'.")
            with st.expander("Ver resumo dos dados"):
                st.dataframe(active_chat["dataframe"].head())
                buffer = io.StringIO()
                active_chat["dataframe"].info(buf=buffer)
                st.text(buffer.getvalue())
            if st.button("üóëÔ∏è Sair do Modo de An√°lise", type="primary", key=f"forget_btn_data_{chat_id}"):
                  create_new_chat()
                  st.rerun()

        elif active_chat.get("contexto_arquivo"):
            st.info("Jarvis est√° em 'Modo de An√°lise de Documento'.")
            st.text_area("Conte√∫do extra√≠do:", value=active_chat["contexto_arquivo"], height=200, key=f"contexto_arquivo_{chat_id}")
            if st.button("üóëÔ∏è Esquecer Arquivo Atual", type="primary", key=f"forget_btn_doc_{chat_id}"):
                create_new_chat()
                st.rerun()

    IS_CLOUD_ENV = os.getenv("STREAMLIT_SERVER_RUN_ON_CLOUD") == "true"

    if not IS_CLOUD_ENV:
        if st.button("üéôÔ∏èFalar", use_container_width=True, key=f"mic_btn_{chat_id}"):
            texto_audio, tom_da_voz = escutar_audio()
            
            if texto_audio:
                active_chat = st.session_state.chats[st.session_state.current_chat_id]
                active_chat["messages"].append(
                    {"role": "user", "type": "text", "content": texto_audio})
                salvar_chats(st.session_state["username"])
                
                processar_entrada_usuario(texto_audio, tom_voz=tom_da_voz)
    else:
        st.sidebar.warning("A fun√ß√£o de microfone est√° desativada na vers√£o web.", icon="üéôÔ∏è")


# --- √ÅREA PRINCIPAL DO CHAT ---
st.write(f"### {active_chat['title']}")

for i, mensagem in enumerate(active_chat["messages"]):
    with st.chat_message(mensagem["role"]):
        # --- NOVA L√ìGICA PARA EXIBIR GR√ÅFICOS ---
        if mensagem.get("type") == "plot":
            st.plotly_chart(mensagem["content"], use_container_width=True)
        # --- FIM DA NOVA L√ìGICA ---
        elif mensagem.get("type") == "image":
            st.image(mensagem["content"], caption=mensagem.get("prompt", "Imagem gerada"))
        else:
            st.write(mensagem["content"]) # L√≥gica existente para texto

# ... no loop principal de chat
# Verifica se a mensagem veio da OpenAI E SE o usu√°rio logado √© o admin
        if mensagem.get("origem") == "openai" and st.session_state.get("username") == ADMIN_USERNAME:
            # Pega a pergunta do usu√°rio que gerou esta resposta
            pergunta_original = active_chat["messages"][i-1]["content"]
            resposta_original = mensagem["content"]

            # Cria colunas para alinhar os √≠cones dos bot√µes
            cols = st.columns([1, 1, 10])  # A √∫ltima coluna √© um espa√ßador

            # Coluna 1: Bot√£o Salvar
            with cols[0]:
                if st.button("‚úÖ", key=f"save_{i}", help="Salvar resposta na mem√≥ria"):
                    adicionar_a_memoria(pergunta_original, resposta_original)
                    mensagem["origem"] = "openai_curado"
                    salvar_chats(st.session_state["username"])
                    st.rerun()

            # Coluna 2: Bot√£o Editar (com Popover)
            with cols[1]:
                with st.popover("‚úèÔ∏è", help="Editar antes de salvar"):
                    with st.form(key=f"edit_form_{i}"):
                        st.write(
                            "Ajuste a pergunta e/ou a resposta antes de salvar.")
                        pergunta_editada = st.text_area(
                            "Pergunta:", value=pergunta_original, height=100)
                        resposta_editada = st.text_area(
                            "Resposta:", value=resposta_original, height=200)
                        if st.form_submit_button("Salvar Edi√ß√£o"):
                            adicionar_a_memoria(
                                pergunta_editada, resposta_editada)
                            mensagem["origem"] = "openai_curado"
                            salvar_chats(st.session_state["username"])
                            st.rerun()

# L√≥gica de Text-to-Speech (continua aqui)
if active_chat["messages"] and active_chat["messages"][-1]["role"] == "assistant" and voz_ativada:
    if active_chat["messages"][-1].get("type") == "text":
        resposta_ia = active_chat["messages"][-1]["content"]
        if resposta_ia != active_chat.get("ultima_mensagem_falada"):
            idioma_detectado = detectar_idioma_com_ia(resposta_ia)
            texto_limpo_para_fala = preparar_texto_para_fala(resposta_ia)
            resposta_formatada_para_voz = json.dumps(texto_limpo_para_fala)
            st.components.v1.html(f"""
            <script>
                function getVoices() {{ return new Promise(resolve => {{ let voices = speechSynthesis.getVoices(); if (voices.length) {{ resolve(voices); return; }} speechSynthesis.onvoiceschanged = () => {{ voices = speechSynthesis.getVoices(); resolve(voices); }}; }}); }}
                async function speak() {{ const text = {resposta_formatada_para_voz}; const idioma = '{idioma_detectado}'; if (!text || text.trim() === '') return; const allVoices = await getVoices(); let voicesForLang = allVoices.filter(v => v.lang.startsWith(idioma)); let desiredVoice; if (voicesForLang.length > 0) {{ if (idioma === 'pt') {{ const ptFemaleNames = ['Microsoft Francisca Online (Natural) - Portuguese (Brazil)', 'Microsoft Maria - Portuguese (Brazil)', 'Google portugu√™s do Brasil', 'Luciana', 'Joana']; for (const name of ptFemaleNames) {{ desiredVoice = voicesForLang.find(v => v.name === name); if (desiredVoice) break; }} }} if (!desiredVoice) {{ const femaleMarkers = ['Female', 'Feminino', 'Femme', 'Mujer']; desiredVoice = voicesForLang.find(v => femaleMarkers.some(marker => v.name.includes(marker))); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => v.default); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => !v.localService); }} if (!desiredVoice) {{ desiredVoice = voicesForLang[0]; }} }} const utterance = new SpeechSynthesisUtterance(text); if (desiredVoice) {{ utterance.voice = desiredVoice; utterance.lang = desiredVoice.lang; }} else {{ utterance.lang = idioma; }} utterance.pitch = 1.0; utterance.rate = 1.0; speechSynthesis.cancel(); speechSynthesis.speak(utterance); }}
                speak();
            </script>
            """, height=0)
            active_chat["ultima_mensagem_falada"] = resposta_ia
            salvar_chats(st.session_state["username"])

if active_chat["messages"] and active_chat["messages"][-1]["role"] == "assistant" and voz_ativada:
    if active_chat["messages"][-1].get("type") == "text":
        resposta_ia = active_chat["messages"][-1]["content"]
        if resposta_ia != active_chat.get("ultima_mensagem_falada"):
            idioma_detectado = detectar_idioma_com_ia(resposta_ia)
            texto_limpo_para_fala = preparar_texto_para_fala(resposta_ia)
            resposta_formatada_para_voz = json.dumps(texto_limpo_para_fala)
            st.components.v1.html(f"""
            <script>
                function getVoices() {{ return new Promise(resolve => {{ let voices = speechSynthesis.getVoices(); if (voices.length) {{ resolve(voices); return; }} speechSynthesis.onvoiceschanged = () => {{ voices = speechSynthesis.getVoices(); resolve(voices); }}; }}); }}
                async function speak() {{ const text = {resposta_formatada_para_voz}; const idioma = '{idioma_detectado}'; if (!text || text.trim() === '') return; const allVoices = await getVoices(); let voicesForLang = allVoices.filter(v => v.lang.startsWith(idioma)); let desiredVoice; if (voicesForLang.length > 0) {{ if (idioma === 'pt') {{ const ptFemaleNames = ['Microsoft Francisca Online (Natural) - Portuguese (Brazil)', 'Microsoft Maria - Portuguese (Brazil)', 'Google portugu√™s do Brasil', 'Luciana', 'Joana']; for (const name of ptFemaleNames) {{ desiredVoice = voicesForLang.find(v => v.name === name); if (desiredVoice) break; }} }} if (!desiredVoice) {{ const femaleMarkers = ['Female', 'Feminino', 'Femme', 'Mujer']; desiredVoice = voicesForLang.find(v => femaleMarkers.some(marker => v.name.includes(marker))); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => v.default); }} if (!desiredVoice) {{ desiredVoice = voicesForLang.find(v => !v.localService); }} if (!desiredVoice) {{ desiredVoice = voicesForLang[0]; }} }} const utterance = new SpeechSynthesisUtterance(text); if (desiredVoice) {{ utterance.voice = desiredVoice; utterance.lang = desiredVoice.lang; }} else {{ utterance.lang = idioma; }} utterance.pitch = 1.0; utterance.rate = 1.0; speechSynthesis.cancel(); speechSynthesis.speak(utterance); }}
                speak();
            </script>
            """, height=0)
            active_chat["ultima_mensagem_falada"] = resposta_ia
            salvar_chats(st.session_state["username"])

# --- Bloco para Exibi√ß√£o Persistente do Bot√£o de Download ---
# Ele verifica em toda recarga se deve mostrar o bot√£o.
if 'pdf_para_download' in st.session_state:
    with st.chat_message("assistant"):
        st.download_button(
            label="üì• Baixar PDF",
            data=st.session_state['pdf_para_download'],
            file_name=st.session_state['pdf_filename'],
            mime="application/pdf",
            on_click=limpar_pdf_da_memoria  # <-- A M√ÅGICA ACONTECE AQUI
        )

# --- ENTRADA DE TEXTO DO USU√ÅRIO ---
if prompt_usuario := st.chat_input("Fale com a Jarvis ou use /lembrese, /imagine, /pdf..."):

    # Adiciona a mensagem do usu√°rio ao hist√≥rico para exibi√ß√£o imediata
    active_chat["messages"].append(
        {"role": "user", "type": "text", "content": prompt_usuario})

    # Salva o chat imediatamente ap√≥s adicionar a mensagem do usu√°rio
    salvar_chats(st.session_state["username"])

    # --- PROCESSAMENTO DE COMANDOS ESPECIAIS ---
    # A estrutura if/elif/else a seguir est√° CORRETAMENTE aninhada.
    if prompt_usuario.lower().startswith("/lembrese "):
        texto_para_lembrar = prompt_usuario[10:].strip()
        if texto_para_lembrar:
            with st.chat_message("assistant"):
                st.info("Memorizando sua prefer√™ncia...")
                processar_comando_lembrese(texto_para_lembrar)

    elif prompt_usuario.lower().startswith("/imagine "):
        prompt_da_imagem = prompt_usuario[9:].strip()
        if prompt_da_imagem:
            with st.chat_message("assistant"):
                url_da_imagem = gerar_imagem_com_dalle(prompt_da_imagem)
                if url_da_imagem:
                    active_chat["messages"].append(
                        {"role": "assistant", "type": "image", "content": url_da_imagem, "prompt": prompt_da_imagem})
                    salvar_chats(st.session_state["username"])
            st.rerun()

    elif prompt_usuario.lower().startswith("/pdf "):
        topico_pdf = prompt_usuario[5:].strip()
        if topico_pdf:
            with st.chat_message("assistant"):
                with st.spinner("Criando seu PDF..."):
                    texto_completo_ia = gerar_conteudo_para_pdf(topico_pdf)
                    
                    linhas_ia = texto_completo_ia.strip().split('\n')
                    titulo_documento = linhas_ia[0].replace('**', '').replace('###', '').replace('##', '').replace('#', '').strip()
                    texto_corpo = '\n'.join(linhas_ia[1:]).strip()
                    
                    pdf_bytes = criar_pdf(texto_corpo, titulo_documento)
                    
                    st.session_state['pdf_para_download'] = pdf_bytes
                    st.session_state['pdf_filename'] = f"{titulo_documento.replace(' ', '_')[:30]}.pdf"

            active_chat["messages"].append(
                {"role": "assistant", "type": "text", "content": f"Criei um PDF sobre '{titulo_documento}'. O bot√£o de download foi exibido."})
            salvar_chats(st.session_state["username"])
        
        st.rerun()

    else:
        # Se n√£o for nenhum comando, chama a fun√ß√£o de processamento de chat normal
        processar_entrada_usuario(prompt_usuario)